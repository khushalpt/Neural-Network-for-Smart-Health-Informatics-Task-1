{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1106937.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66AWsiLqLi1N",
        "colab_type": "text"
      },
      "source": [
        "Importing necessary libraries to read the file, and format the dataset using pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTwLO4HEYUQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from random import shuffle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import operator\n",
        "import matplotlib.pyplot as plt\n",
        "import os, glob"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1dSXZYXJoEJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "5464b274-5a8f-4f7b-c32e-3ef01392cba8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvBvzg5fYaH-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "5439febd-a016-4d24-9a1a-f52e61cfd86e"
      },
      "source": [
        "#Adding the path of dataset uploaded on the drive to the variable path\n",
        "dataView = pd.read_csv(\"/content/drive/My Drive/DS/data0.csv\")\n",
        "dataView"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>OptimizedValue:1606.0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[156 499 284  25 300  40 346 108 190 458 358 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>449 225 487  54 497 420 207 204 175 423 301 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>345 435 295 443 397  63  35 491 264 113 179 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[219 400  62  99  34 432 259 315 319  88 378 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>320 418 230 294 334 312 313 121 146 330 131 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>495 251 464 290 279 330 251 327 319  24 466 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>243  72  68 334  33  79  96 443 387 101 387 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>[ 72 354 129 489 136 101 375 244 282  34 355 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>77  89 366 240 419 225 325 136 214  24  85 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>94 395 468  25 315 348 312 420 295 327  68 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 OptimizedValue:1606.0\n",
              "0    [[156 499 284  25 300  40 346 108 190 458 358 ...\n",
              "1      449 225 487  54 497 420 207 204 175 423 301 ...\n",
              "2      345 435 295 443 397  63  35 491 264 113 179 ...\n",
              "3     [219 400  62  99  34 432 259 315 319  88 378 ...\n",
              "4      320 418 230 294 334 312 313 121 146 330 131 ...\n",
              "..                                                 ...\n",
              "145    495 251 464 290 279 330 251 327 319  24 466 ...\n",
              "146    243  72  68 334  33  79  96 443 387 101 387 ...\n",
              "147   [ 72 354 129 489 136 101 375 244 282  34 355 ...\n",
              "148     77  89 366 240 419 225 325 136 214  24  85 ...\n",
              "149     94 395 468  25 315 348 312 420 295 327  68 ...\n",
              "\n",
              "[150 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4rRMTxSMH2-",
        "colab_type": "text"
      },
      "source": [
        "Creating a function reformat() that processes the provided data set.\n",
        "\n",
        "*   Removes the brackets \"[\", \"]\"\n",
        "*   Split the data when \" \" is encountered\n",
        "\n",
        "\n",
        "Preprocessing the data set by calling the reformat() and also segregating \"OptimizedValue\" from the file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV15DILlJiNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Row 'rowValue' is the argument for the function reformat \n",
        "# rowArray is intialised as an array that takes in as input the data after reformatting\n",
        "def reformat(rowValue):\n",
        "      list = []\n",
        "\n",
        "      # data_op = c.split(\":\")\n",
        "      rowValue = rowValue.strip(\" ] \")\n",
        "      rowValue = rowValue.strip(\" [ \")\n",
        "\n",
        "      # Reformating the dataset by removing \" \"\n",
        "      rowValue = rowValue.split(\" \") \n",
        "\n",
        "      for string in rowValue:\n",
        "        if (string != \"\"):\n",
        "          # Appending the actual values i.e., time taken by each doctor\n",
        "          list.append(string) \n",
        "      return list\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl3xkMxMOCSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a for loop for 1000 data files provided\n",
        "for file_count in range(0,1000):\n",
        "\n",
        "  # Creating a panda dataframe by reading csv files from the path \n",
        "  dataset = pd.read_csv(\"/content/drive/My Drive/DS/data\"+ str(file_count) +\".csv\")\n",
        "  columnValue = dataset.columns[0] #initialising the first column\n",
        "  \n",
        "  # splitting the value with \":\" so that the actual optimized value can be procured\n",
        "  data_op = columnValue.split(\":\")\n",
        "  for optimizedVal in data_op:\n",
        "    if(optimizedVal != \"OptimizedValue\"):\n",
        "      val_data_ann = int(float(optimizedVal))\n",
        "      val_data_cnn = int(float(optimizedVal))\n",
        "  \n",
        "  # creating a list that contains row values i.e., the time taken by each doctor\n",
        "  list1_ann = []\n",
        "  list1_cnn = []\n",
        "  for i in range(0,150):\n",
        "    arrayData = dataset[columnValue][i]\n",
        "    reformatvalue = reformat(arrayData)\n",
        "   \n",
        "    for i in reformatvalue:\n",
        "      list1_ann.append(int(i))\n",
        "      list1_cnn.append(int(i))\n",
        "\n",
        "  # Creating two lists for ANN and CNN model which will be initailised and used later\n",
        "  list2_ann=[]\n",
        "  list2_cnn=[]\n",
        "  \n",
        "  list1_ann = [float(i)/sum(list1_ann) for i in list1_ann]\n",
        "  list2_ann.append(list1_ann)\n",
        "  list1_cnn = [float(i)/sum(list1_cnn) for i in list1_cnn]\n",
        "  list2_cnn.append(list1_cnn)\n",
        "\n",
        "  # df1_ann = pd.DataFrame({\"header_data_ann\" : list2_ann ,\"val_data_ann\" : val_data_ann})\n",
        "  # df1_cnn = pd.DataFrame({\"header_data_cnn\" : list2_cnn ,\"val_data_cnn\" : val_data_cnn})\n",
        "  \n",
        "  #Creating a condition that enters the optimized value to the val_data and time taken to header_data(list_2)\n",
        "  if( file_count == 0 ):\n",
        "    df1_ann = pd.DataFrame({\"header_data_ann\" : list2_ann ,\"val_data_ann\" : val_data_ann})\n",
        "    df1_cnn = pd.DataFrame({\"header_data_cnn\" : list2_cnn ,\"val_data_cnn\" : val_data_cnn})\n",
        "  else:\n",
        "    df2_ann = pd.DataFrame({\"header_data_ann\" : list2_ann ,\"val_data_ann\" : val_data_ann})\n",
        "    df1_ann = df1_ann.append(df2_ann, ignore_index= True)\n",
        "    df2_cnn = pd.DataFrame({\"header_data_cnn\" : list2_cnn ,\"val_data_cnn\" : val_data_cnn})\n",
        "    df1_cnn = df1_cnn.append(df2_cnn, ignore_index= True)\n",
        "  "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp8_mqhDdM6e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "a39f1830-711a-4c09-9b3b-b96da27195f8"
      },
      "source": [
        "df1_ann.describe"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.describe of                                        header_data_ann  val_data_ann\n",
              "0    [0.00023981515785525311, 0.0007671010498062263...          1606\n",
              "1    [0.000132112238783435, 7.234717838140489e-05, ...          1714\n",
              "2    [0.00010460942992475813, 0.0006153495877926949...          1936\n",
              "3    [0.0006157502763179365, 0.0002278276022376365,...          1811\n",
              "4    [4.323997102921941e-05, 0.0004339439949718091,...          1890\n",
              "..                                                 ...           ...\n",
              "995  [0.0005424143047744928, 0.0004909784655286357,...          1729\n",
              "996  [0.0002588882425143226, 0.00029696004288407593...          1819\n",
              "997  [0.0005328214923364066, 0.0006247409895523365,...          1721\n",
              "998  [0.0004902576610419814, 0.0007614314298058273,...          1827\n",
              "999  [0.0007320542030302701, 0.000266765514663573, ...          1744\n",
              "\n",
              "[1000 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVsZKxV-SBbQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "a718db1b-cc53-49be-a0ef-ef490a4455ea"
      },
      "source": [
        "df1_cnn.describe"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.describe of                                        header_data_cnn  val_data_cnn\n",
              "0    [0.00023981515785525311, 0.0007671010498062263...          1606\n",
              "1    [0.000132112238783435, 7.234717838140489e-05, ...          1714\n",
              "2    [0.00010460942992475813, 0.0006153495877926949...          1936\n",
              "3    [0.0006157502763179365, 0.0002278276022376365,...          1811\n",
              "4    [4.323997102921941e-05, 0.0004339439949718091,...          1890\n",
              "..                                                 ...           ...\n",
              "995  [0.0005424143047744928, 0.0004909784655286357,...          1729\n",
              "996  [0.0002588882425143226, 0.00029696004288407593...          1819\n",
              "997  [0.0005328214923364066, 0.0006247409895523365,...          1721\n",
              "998  [0.0004902576610419814, 0.0007614314298058273,...          1827\n",
              "999  [0.0007320542030302701, 0.000266765514663573, ...          1744\n",
              "\n",
              "[1000 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_mRZhG5dcmV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "b17e8723-d3de-4c56-db39-fb0fdf28604d"
      },
      "source": [
        "l1_ann = df1_ann['header_data_ann'].apply(pd.Series)\n",
        "l1_cnn = df1_cnn['header_data_cnn'].apply(pd.Series)\n",
        "l1_ann"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2460</th>\n",
              "      <th>2461</th>\n",
              "      <th>2462</th>\n",
              "      <th>2463</th>\n",
              "      <th>2464</th>\n",
              "      <th>2465</th>\n",
              "      <th>2466</th>\n",
              "      <th>2467</th>\n",
              "      <th>2468</th>\n",
              "      <th>2469</th>\n",
              "      <th>2470</th>\n",
              "      <th>2471</th>\n",
              "      <th>2472</th>\n",
              "      <th>2473</th>\n",
              "      <th>2474</th>\n",
              "      <th>2475</th>\n",
              "      <th>2476</th>\n",
              "      <th>2477</th>\n",
              "      <th>2478</th>\n",
              "      <th>2479</th>\n",
              "      <th>2480</th>\n",
              "      <th>2481</th>\n",
              "      <th>2482</th>\n",
              "      <th>2483</th>\n",
              "      <th>2484</th>\n",
              "      <th>2485</th>\n",
              "      <th>2486</th>\n",
              "      <th>2487</th>\n",
              "      <th>2488</th>\n",
              "      <th>2489</th>\n",
              "      <th>2490</th>\n",
              "      <th>2491</th>\n",
              "      <th>2492</th>\n",
              "      <th>2493</th>\n",
              "      <th>2494</th>\n",
              "      <th>2495</th>\n",
              "      <th>2496</th>\n",
              "      <th>2497</th>\n",
              "      <th>2498</th>\n",
              "      <th>2499</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000240</td>\n",
              "      <td>0.000767</td>\n",
              "      <td>0.000437</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000461</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.000532</td>\n",
              "      <td>0.000166</td>\n",
              "      <td>0.000292</td>\n",
              "      <td>0.000704</td>\n",
              "      <td>0.000550</td>\n",
              "      <td>0.000492</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>0.000143</td>\n",
              "      <td>0.000435</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.000420</td>\n",
              "      <td>0.000690</td>\n",
              "      <td>0.000346</td>\n",
              "      <td>0.000749</td>\n",
              "      <td>0.000083</td>\n",
              "      <td>0.000764</td>\n",
              "      <td>0.000646</td>\n",
              "      <td>0.000318</td>\n",
              "      <td>0.000314</td>\n",
              "      <td>0.000269</td>\n",
              "      <td>0.000650</td>\n",
              "      <td>0.000463</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.000470</td>\n",
              "      <td>0.000214</td>\n",
              "      <td>0.000544</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>0.000501</td>\n",
              "      <td>0.000121</td>\n",
              "      <td>0.000530</td>\n",
              "      <td>0.000669</td>\n",
              "      <td>0.000453</td>\n",
              "      <td>0.000681</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000546</td>\n",
              "      <td>0.000486</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000363</td>\n",
              "      <td>0.000450</td>\n",
              "      <td>0.000354</td>\n",
              "      <td>0.000315</td>\n",
              "      <td>0.000744</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000563</td>\n",
              "      <td>0.000369</td>\n",
              "      <td>0.000644</td>\n",
              "      <td>0.000346</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.000209</td>\n",
              "      <td>0.000329</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>0.000726</td>\n",
              "      <td>0.000735</td>\n",
              "      <td>0.000723</td>\n",
              "      <td>0.000570</td>\n",
              "      <td>0.000237</td>\n",
              "      <td>0.000630</td>\n",
              "      <td>0.000575</td>\n",
              "      <td>0.000145</td>\n",
              "      <td>0.000607</td>\n",
              "      <td>0.000719</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000484</td>\n",
              "      <td>0.000535</td>\n",
              "      <td>0.000480</td>\n",
              "      <td>0.000646</td>\n",
              "      <td>0.000453</td>\n",
              "      <td>0.000503</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>0.000417</td>\n",
              "      <td>0.000464</td>\n",
              "      <td>0.000072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000132</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.000428</td>\n",
              "      <td>0.000082</td>\n",
              "      <td>0.000517</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.000609</td>\n",
              "      <td>0.000168</td>\n",
              "      <td>0.000530</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>0.000313</td>\n",
              "      <td>0.000617</td>\n",
              "      <td>0.000760</td>\n",
              "      <td>0.000241</td>\n",
              "      <td>0.000439</td>\n",
              "      <td>0.000560</td>\n",
              "      <td>0.000506</td>\n",
              "      <td>0.000241</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>0.000275</td>\n",
              "      <td>0.000318</td>\n",
              "      <td>0.000632</td>\n",
              "      <td>0.000631</td>\n",
              "      <td>0.000670</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.000455</td>\n",
              "      <td>0.000532</td>\n",
              "      <td>0.000409</td>\n",
              "      <td>0.000613</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000459</td>\n",
              "      <td>0.000406</td>\n",
              "      <td>0.000546</td>\n",
              "      <td>0.000583</td>\n",
              "      <td>0.000140</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000709</td>\n",
              "      <td>0.000698</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000535</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.000445</td>\n",
              "      <td>0.000618</td>\n",
              "      <td>0.000248</td>\n",
              "      <td>0.000264</td>\n",
              "      <td>0.000396</td>\n",
              "      <td>0.000565</td>\n",
              "      <td>0.000228</td>\n",
              "      <td>0.000469</td>\n",
              "      <td>0.000448</td>\n",
              "      <td>0.000664</td>\n",
              "      <td>0.000667</td>\n",
              "      <td>0.000395</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>0.000596</td>\n",
              "      <td>0.000393</td>\n",
              "      <td>0.000338</td>\n",
              "      <td>0.000374</td>\n",
              "      <td>0.000544</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>0.000569</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.000632</td>\n",
              "      <td>0.000745</td>\n",
              "      <td>0.000645</td>\n",
              "      <td>0.000415</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.000247</td>\n",
              "      <td>0.000698</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000656</td>\n",
              "      <td>0.000613</td>\n",
              "      <td>0.000104</td>\n",
              "      <td>0.000392</td>\n",
              "      <td>0.000448</td>\n",
              "      <td>0.000241</td>\n",
              "      <td>0.000777</td>\n",
              "      <td>0.000351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000105</td>\n",
              "      <td>0.000615</td>\n",
              "      <td>0.000243</td>\n",
              "      <td>0.000545</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.000721</td>\n",
              "      <td>0.000712</td>\n",
              "      <td>0.000448</td>\n",
              "      <td>0.000228</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>0.000580</td>\n",
              "      <td>0.000451</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>0.000442</td>\n",
              "      <td>0.000712</td>\n",
              "      <td>0.000735</td>\n",
              "      <td>0.000766</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>0.000318</td>\n",
              "      <td>0.000518</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.000503</td>\n",
              "      <td>0.000535</td>\n",
              "      <td>0.000285</td>\n",
              "      <td>0.000308</td>\n",
              "      <td>0.000231</td>\n",
              "      <td>0.000566</td>\n",
              "      <td>0.000689</td>\n",
              "      <td>0.000755</td>\n",
              "      <td>0.000338</td>\n",
              "      <td>0.000637</td>\n",
              "      <td>0.000422</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.000577</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000423</td>\n",
              "      <td>0.000465</td>\n",
              "      <td>0.000718</td>\n",
              "      <td>0.000595</td>\n",
              "      <td>0.000252</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000743</td>\n",
              "      <td>0.000174</td>\n",
              "      <td>0.000723</td>\n",
              "      <td>0.000683</td>\n",
              "      <td>0.000378</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>0.000180</td>\n",
              "      <td>0.000362</td>\n",
              "      <td>0.000315</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000492</td>\n",
              "      <td>0.000215</td>\n",
              "      <td>0.000635</td>\n",
              "      <td>0.000617</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000323</td>\n",
              "      <td>0.000377</td>\n",
              "      <td>0.000123</td>\n",
              "      <td>0.000646</td>\n",
              "      <td>0.000238</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.000471</td>\n",
              "      <td>0.000748</td>\n",
              "      <td>0.000443</td>\n",
              "      <td>0.000475</td>\n",
              "      <td>0.000712</td>\n",
              "      <td>0.000725</td>\n",
              "      <td>0.000397</td>\n",
              "      <td>0.000226</td>\n",
              "      <td>0.000317</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000755</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>0.000458</td>\n",
              "      <td>0.000357</td>\n",
              "      <td>0.000278</td>\n",
              "      <td>0.000620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000616</td>\n",
              "      <td>0.000228</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000182</td>\n",
              "      <td>0.000720</td>\n",
              "      <td>0.000528</td>\n",
              "      <td>0.000294</td>\n",
              "      <td>0.000671</td>\n",
              "      <td>0.000285</td>\n",
              "      <td>0.000386</td>\n",
              "      <td>0.000705</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.000309</td>\n",
              "      <td>0.000596</td>\n",
              "      <td>0.000249</td>\n",
              "      <td>0.000577</td>\n",
              "      <td>0.000568</td>\n",
              "      <td>0.000342</td>\n",
              "      <td>0.000405</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.000702</td>\n",
              "      <td>0.000702</td>\n",
              "      <td>0.000269</td>\n",
              "      <td>0.000659</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000676</td>\n",
              "      <td>0.000322</td>\n",
              "      <td>0.000272</td>\n",
              "      <td>0.000328</td>\n",
              "      <td>0.000519</td>\n",
              "      <td>0.000429</td>\n",
              "      <td>0.000657</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000605</td>\n",
              "      <td>0.000526</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000688</td>\n",
              "      <td>0.000125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000468</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000708</td>\n",
              "      <td>0.000657</td>\n",
              "      <td>0.000491</td>\n",
              "      <td>0.000440</td>\n",
              "      <td>0.000366</td>\n",
              "      <td>0.000229</td>\n",
              "      <td>0.000483</td>\n",
              "      <td>0.000656</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.000588</td>\n",
              "      <td>0.000682</td>\n",
              "      <td>0.000216</td>\n",
              "      <td>0.000140</td>\n",
              "      <td>0.000653</td>\n",
              "      <td>0.000764</td>\n",
              "      <td>0.000696</td>\n",
              "      <td>0.000724</td>\n",
              "      <td>0.000120</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.000168</td>\n",
              "      <td>0.000353</td>\n",
              "      <td>0.000075</td>\n",
              "      <td>0.000727</td>\n",
              "      <td>0.000716</td>\n",
              "      <td>0.000654</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>0.000312</td>\n",
              "      <td>0.000319</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.000429</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.000140</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.000095</td>\n",
              "      <td>0.000271</td>\n",
              "      <td>0.000616</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.000434</td>\n",
              "      <td>0.000249</td>\n",
              "      <td>0.000349</td>\n",
              "      <td>0.000103</td>\n",
              "      <td>0.000119</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>0.000335</td>\n",
              "      <td>0.000650</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>0.000757</td>\n",
              "      <td>0.000366</td>\n",
              "      <td>0.000414</td>\n",
              "      <td>0.000471</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>0.000059</td>\n",
              "      <td>0.000434</td>\n",
              "      <td>0.000619</td>\n",
              "      <td>0.000587</td>\n",
              "      <td>0.000349</td>\n",
              "      <td>0.000590</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.000252</td>\n",
              "      <td>0.000545</td>\n",
              "      <td>0.000599</td>\n",
              "      <td>0.000540</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>0.000689</td>\n",
              "      <td>0.000622</td>\n",
              "      <td>0.000531</td>\n",
              "      <td>0.000292</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000561</td>\n",
              "      <td>0.000286</td>\n",
              "      <td>0.000253</td>\n",
              "      <td>0.000449</td>\n",
              "      <td>0.000579</td>\n",
              "      <td>0.000195</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000547</td>\n",
              "      <td>0.000320</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>0.000295</td>\n",
              "      <td>0.000088</td>\n",
              "      <td>0.000462</td>\n",
              "      <td>0.000168</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.000479</td>\n",
              "      <td>0.000625</td>\n",
              "      <td>0.000703</td>\n",
              "      <td>0.000666</td>\n",
              "      <td>0.000338</td>\n",
              "      <td>0.000724</td>\n",
              "      <td>0.000411</td>\n",
              "      <td>0.000040</td>\n",
              "      <td>0.000059</td>\n",
              "      <td>0.000652</td>\n",
              "      <td>0.000587</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>0.000491</td>\n",
              "      <td>0.000658</td>\n",
              "      <td>0.000385</td>\n",
              "      <td>0.000244</td>\n",
              "      <td>0.000292</td>\n",
              "      <td>0.000746</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>0.000644</td>\n",
              "      <td>0.000679</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000692</td>\n",
              "      <td>0.000381</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.000440</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>0.000613</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.000578</td>\n",
              "      <td>0.000318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>0.000542</td>\n",
              "      <td>0.000491</td>\n",
              "      <td>0.000641</td>\n",
              "      <td>0.000260</td>\n",
              "      <td>0.000396</td>\n",
              "      <td>0.000321</td>\n",
              "      <td>0.000318</td>\n",
              "      <td>0.000672</td>\n",
              "      <td>0.000588</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000447</td>\n",
              "      <td>0.000493</td>\n",
              "      <td>0.000561</td>\n",
              "      <td>0.000742</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.000376</td>\n",
              "      <td>0.000379</td>\n",
              "      <td>0.000535</td>\n",
              "      <td>0.000171</td>\n",
              "      <td>0.000444</td>\n",
              "      <td>0.000778</td>\n",
              "      <td>0.000603</td>\n",
              "      <td>0.000644</td>\n",
              "      <td>0.000249</td>\n",
              "      <td>0.000362</td>\n",
              "      <td>0.000491</td>\n",
              "      <td>0.000614</td>\n",
              "      <td>0.000360</td>\n",
              "      <td>0.000281</td>\n",
              "      <td>0.000401</td>\n",
              "      <td>0.000399</td>\n",
              "      <td>0.000363</td>\n",
              "      <td>0.000380</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.000296</td>\n",
              "      <td>0.000574</td>\n",
              "      <td>0.000544</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000561</td>\n",
              "      <td>0.000586</td>\n",
              "      <td>0.000287</td>\n",
              "      <td>0.000708</td>\n",
              "      <td>0.000684</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.000539</td>\n",
              "      <td>0.000316</td>\n",
              "      <td>0.000083</td>\n",
              "      <td>0.000387</td>\n",
              "      <td>0.000327</td>\n",
              "      <td>0.000257</td>\n",
              "      <td>0.000228</td>\n",
              "      <td>0.000588</td>\n",
              "      <td>0.000343</td>\n",
              "      <td>0.000722</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.000095</td>\n",
              "      <td>0.000616</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000132</td>\n",
              "      <td>0.000574</td>\n",
              "      <td>0.000731</td>\n",
              "      <td>0.000385</td>\n",
              "      <td>0.000742</td>\n",
              "      <td>0.000726</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.000075</td>\n",
              "      <td>0.000614</td>\n",
              "      <td>0.000653</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.000726</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000768</td>\n",
              "      <td>0.000357</td>\n",
              "      <td>0.000659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>0.000259</td>\n",
              "      <td>0.000297</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000171</td>\n",
              "      <td>0.000334</td>\n",
              "      <td>0.000665</td>\n",
              "      <td>0.000390</td>\n",
              "      <td>0.000174</td>\n",
              "      <td>0.000445</td>\n",
              "      <td>0.000244</td>\n",
              "      <td>0.000251</td>\n",
              "      <td>0.000347</td>\n",
              "      <td>0.000586</td>\n",
              "      <td>0.000429</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.000349</td>\n",
              "      <td>0.000215</td>\n",
              "      <td>0.000725</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>0.000726</td>\n",
              "      <td>0.000352</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>0.000330</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>0.000396</td>\n",
              "      <td>0.000673</td>\n",
              "      <td>0.000481</td>\n",
              "      <td>0.000362</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>0.000267</td>\n",
              "      <td>0.000432</td>\n",
              "      <td>0.000658</td>\n",
              "      <td>0.000635</td>\n",
              "      <td>0.000697</td>\n",
              "      <td>0.000399</td>\n",
              "      <td>0.000292</td>\n",
              "      <td>0.000542</td>\n",
              "      <td>0.000458</td>\n",
              "      <td>0.000075</td>\n",
              "      <td>0.000448</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000379</td>\n",
              "      <td>0.000571</td>\n",
              "      <td>0.000591</td>\n",
              "      <td>0.000707</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>0.000362</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>0.000378</td>\n",
              "      <td>0.000215</td>\n",
              "      <td>0.000119</td>\n",
              "      <td>0.000760</td>\n",
              "      <td>0.000461</td>\n",
              "      <td>0.000602</td>\n",
              "      <td>0.000390</td>\n",
              "      <td>0.000174</td>\n",
              "      <td>0.000726</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.000693</td>\n",
              "      <td>0.000733</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000172</td>\n",
              "      <td>0.000725</td>\n",
              "      <td>0.000519</td>\n",
              "      <td>0.000641</td>\n",
              "      <td>0.000629</td>\n",
              "      <td>0.000733</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>0.000704</td>\n",
              "      <td>0.000178</td>\n",
              "      <td>0.000594</td>\n",
              "      <td>0.000530</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000653</td>\n",
              "      <td>0.000640</td>\n",
              "      <td>0.000478</td>\n",
              "      <td>0.000091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>0.000533</td>\n",
              "      <td>0.000625</td>\n",
              "      <td>0.000174</td>\n",
              "      <td>0.000748</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000241</td>\n",
              "      <td>0.000615</td>\n",
              "      <td>0.000612</td>\n",
              "      <td>0.000611</td>\n",
              "      <td>0.000310</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>0.000651</td>\n",
              "      <td>0.000385</td>\n",
              "      <td>0.000650</td>\n",
              "      <td>0.000604</td>\n",
              "      <td>0.000432</td>\n",
              "      <td>0.000413</td>\n",
              "      <td>0.000117</td>\n",
              "      <td>0.000270</td>\n",
              "      <td>0.000136</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>0.000623</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>0.000442</td>\n",
              "      <td>0.000182</td>\n",
              "      <td>0.000418</td>\n",
              "      <td>0.000248</td>\n",
              "      <td>0.000488</td>\n",
              "      <td>0.000752</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.000224</td>\n",
              "      <td>0.000774</td>\n",
              "      <td>0.000249</td>\n",
              "      <td>0.000273</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.000586</td>\n",
              "      <td>0.000123</td>\n",
              "      <td>0.000405</td>\n",
              "      <td>0.000570</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>0.000379</td>\n",
              "      <td>0.000154</td>\n",
              "      <td>0.000527</td>\n",
              "      <td>0.000421</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.000284</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000327</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.000231</td>\n",
              "      <td>0.000374</td>\n",
              "      <td>0.000653</td>\n",
              "      <td>0.000215</td>\n",
              "      <td>0.000548</td>\n",
              "      <td>0.000422</td>\n",
              "      <td>0.000531</td>\n",
              "      <td>0.000385</td>\n",
              "      <td>0.000709</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.000612</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000735</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.000637</td>\n",
              "      <td>0.000254</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.000143</td>\n",
              "      <td>0.000346</td>\n",
              "      <td>0.000544</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.000344</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.000492</td>\n",
              "      <td>0.000704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>0.000490</td>\n",
              "      <td>0.000761</td>\n",
              "      <td>0.000357</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>0.000438</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.000406</td>\n",
              "      <td>0.000432</td>\n",
              "      <td>0.000401</td>\n",
              "      <td>0.000326</td>\n",
              "      <td>0.000628</td>\n",
              "      <td>0.000381</td>\n",
              "      <td>0.000686</td>\n",
              "      <td>0.000604</td>\n",
              "      <td>0.000331</td>\n",
              "      <td>0.000423</td>\n",
              "      <td>0.000542</td>\n",
              "      <td>0.000423</td>\n",
              "      <td>0.000472</td>\n",
              "      <td>0.000182</td>\n",
              "      <td>0.000633</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>0.000489</td>\n",
              "      <td>0.000650</td>\n",
              "      <td>0.000339</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>0.000598</td>\n",
              "      <td>0.000346</td>\n",
              "      <td>0.000486</td>\n",
              "      <td>0.000521</td>\n",
              "      <td>0.000115</td>\n",
              "      <td>0.000555</td>\n",
              "      <td>0.000473</td>\n",
              "      <td>0.000519</td>\n",
              "      <td>0.000484</td>\n",
              "      <td>0.000671</td>\n",
              "      <td>0.000522</td>\n",
              "      <td>0.000576</td>\n",
              "      <td>0.000317</td>\n",
              "      <td>0.000340</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>0.000369</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>0.000761</td>\n",
              "      <td>0.000179</td>\n",
              "      <td>0.000294</td>\n",
              "      <td>0.000297</td>\n",
              "      <td>0.000417</td>\n",
              "      <td>0.000244</td>\n",
              "      <td>0.000760</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000640</td>\n",
              "      <td>0.000334</td>\n",
              "      <td>0.000746</td>\n",
              "      <td>0.000090</td>\n",
              "      <td>0.000316</td>\n",
              "      <td>0.000423</td>\n",
              "      <td>0.000533</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>0.000290</td>\n",
              "      <td>0.000651</td>\n",
              "      <td>0.000257</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>0.000231</td>\n",
              "      <td>0.000650</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>0.000455</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>0.000132</td>\n",
              "      <td>0.000501</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000406</td>\n",
              "      <td>0.000363</td>\n",
              "      <td>0.000552</td>\n",
              "      <td>0.000241</td>\n",
              "      <td>0.000492</td>\n",
              "      <td>0.000674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>0.000732</td>\n",
              "      <td>0.000267</td>\n",
              "      <td>0.000129</td>\n",
              "      <td>0.000741</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>0.000461</td>\n",
              "      <td>0.000290</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.000257</td>\n",
              "      <td>0.000310</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.000512</td>\n",
              "      <td>0.000591</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>0.000639</td>\n",
              "      <td>0.000395</td>\n",
              "      <td>0.000226</td>\n",
              "      <td>0.000402</td>\n",
              "      <td>0.000181</td>\n",
              "      <td>0.000746</td>\n",
              "      <td>0.000758</td>\n",
              "      <td>0.000343</td>\n",
              "      <td>0.000292</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.000687</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000413</td>\n",
              "      <td>0.000366</td>\n",
              "      <td>0.000413</td>\n",
              "      <td>0.000586</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>0.000464</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>0.000501</td>\n",
              "      <td>0.000489</td>\n",
              "      <td>0.000292</td>\n",
              "      <td>0.000278</td>\n",
              "      <td>0.000129</td>\n",
              "      <td>0.000442</td>\n",
              "      <td>0.000253</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000473</td>\n",
              "      <td>0.000279</td>\n",
              "      <td>0.000743</td>\n",
              "      <td>0.000248</td>\n",
              "      <td>0.000439</td>\n",
              "      <td>0.000715</td>\n",
              "      <td>0.000712</td>\n",
              "      <td>0.000569</td>\n",
              "      <td>0.000437</td>\n",
              "      <td>0.000476</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.000617</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000489</td>\n",
              "      <td>0.000203</td>\n",
              "      <td>0.000371</td>\n",
              "      <td>0.000561</td>\n",
              "      <td>0.000580</td>\n",
              "      <td>0.000529</td>\n",
              "      <td>0.000409</td>\n",
              "      <td>0.000451</td>\n",
              "      <td>0.000453</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.000448</td>\n",
              "      <td>0.000382</td>\n",
              "      <td>0.000706</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>0.000741</td>\n",
              "      <td>0.000741</td>\n",
              "      <td>0.000420</td>\n",
              "      <td>0.000268</td>\n",
              "      <td>0.000738</td>\n",
              "      <td>0.000226</td>\n",
              "      <td>0.000468</td>\n",
              "      <td>0.000693</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000487</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2500 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2     ...      2497      2498      2499\n",
              "0    0.000240  0.000767  0.000437  ...  0.000417  0.000464  0.000072\n",
              "1    0.000132  0.000072  0.000428  ...  0.000241  0.000777  0.000351\n",
              "2    0.000105  0.000615  0.000243  ...  0.000357  0.000278  0.000620\n",
              "3    0.000616  0.000228  0.000037  ...  0.000271  0.000616  0.000700\n",
              "4    0.000043  0.000434  0.000249  ...  0.000094  0.000578  0.000318\n",
              "..        ...       ...       ...  ...       ...       ...       ...\n",
              "995  0.000542  0.000491  0.000641  ...  0.000768  0.000357  0.000659\n",
              "996  0.000259  0.000297  0.000037  ...  0.000640  0.000478  0.000091\n",
              "997  0.000533  0.000625  0.000174  ...  0.000218  0.000492  0.000704\n",
              "998  0.000490  0.000761  0.000357  ...  0.000241  0.000492  0.000674\n",
              "999  0.000732  0.000267  0.000129  ...  0.000693  0.000138  0.000487\n",
              "\n",
              "[1000 rows x 2500 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0amsq06YPYIo",
        "colab_type": "text"
      },
      "source": [
        "Importing necessary libraries to:\n",
        "\n",
        "\n",
        "*   Scale the source dataset\n",
        "*   split the dataset into test and train\n",
        "*   create and run the CNN keras model\n",
        "*   Perform K fold cross validation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ow82cYJeBFV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e1210361-67ed-450e-ecda-cc0b58a81240"
      },
      "source": [
        "from keras.layers import ReLU\n",
        "import keras\n",
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRrJBqDaeEu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting the test and train into 30 and 70 ratio\n",
        "x_train_ann, x_test_ann = train_test_split(l1_ann, train_size = 0.7)\n",
        "y_train_ann, y_test_ann = train_test_split(df1_ann.val_data_ann, train_size = 0.7)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCPytkZzeYd3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "899af2ed-0788-4dd2-848c-3c987e57c444"
      },
      "source": [
        "x_train_ann"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2460</th>\n",
              "      <th>2461</th>\n",
              "      <th>2462</th>\n",
              "      <th>2463</th>\n",
              "      <th>2464</th>\n",
              "      <th>2465</th>\n",
              "      <th>2466</th>\n",
              "      <th>2467</th>\n",
              "      <th>2468</th>\n",
              "      <th>2469</th>\n",
              "      <th>2470</th>\n",
              "      <th>2471</th>\n",
              "      <th>2472</th>\n",
              "      <th>2473</th>\n",
              "      <th>2474</th>\n",
              "      <th>2475</th>\n",
              "      <th>2476</th>\n",
              "      <th>2477</th>\n",
              "      <th>2478</th>\n",
              "      <th>2479</th>\n",
              "      <th>2480</th>\n",
              "      <th>2481</th>\n",
              "      <th>2482</th>\n",
              "      <th>2483</th>\n",
              "      <th>2484</th>\n",
              "      <th>2485</th>\n",
              "      <th>2486</th>\n",
              "      <th>2487</th>\n",
              "      <th>2488</th>\n",
              "      <th>2489</th>\n",
              "      <th>2490</th>\n",
              "      <th>2491</th>\n",
              "      <th>2492</th>\n",
              "      <th>2493</th>\n",
              "      <th>2494</th>\n",
              "      <th>2495</th>\n",
              "      <th>2496</th>\n",
              "      <th>2497</th>\n",
              "      <th>2498</th>\n",
              "      <th>2499</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>851</th>\n",
              "      <td>0.000549</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.000653</td>\n",
              "      <td>0.000561</td>\n",
              "      <td>0.000726</td>\n",
              "      <td>0.000570</td>\n",
              "      <td>0.000532</td>\n",
              "      <td>0.000731</td>\n",
              "      <td>0.000592</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>0.000570</td>\n",
              "      <td>0.000403</td>\n",
              "      <td>0.000383</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000365</td>\n",
              "      <td>0.000406</td>\n",
              "      <td>0.000744</td>\n",
              "      <td>0.000390</td>\n",
              "      <td>0.000453</td>\n",
              "      <td>0.000387</td>\n",
              "      <td>0.000282</td>\n",
              "      <td>0.000290</td>\n",
              "      <td>0.000297</td>\n",
              "      <td>0.000516</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>0.000381</td>\n",
              "      <td>0.000482</td>\n",
              "      <td>0.000741</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>0.000499</td>\n",
              "      <td>0.000648</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>0.000464</td>\n",
              "      <td>0.000267</td>\n",
              "      <td>0.000511</td>\n",
              "      <td>0.000484</td>\n",
              "      <td>0.000656</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000593</td>\n",
              "      <td>0.000718</td>\n",
              "      <td>0.000662</td>\n",
              "      <td>0.000488</td>\n",
              "      <td>0.000290</td>\n",
              "      <td>0.000734</td>\n",
              "      <td>0.000421</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000477</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.000746</td>\n",
              "      <td>0.000351</td>\n",
              "      <td>0.000358</td>\n",
              "      <td>0.000525</td>\n",
              "      <td>0.000220</td>\n",
              "      <td>0.000363</td>\n",
              "      <td>0.000607</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>0.000720</td>\n",
              "      <td>0.000110</td>\n",
              "      <td>0.000750</td>\n",
              "      <td>0.000121</td>\n",
              "      <td>0.000522</td>\n",
              "      <td>0.000641</td>\n",
              "      <td>0.000252</td>\n",
              "      <td>0.000253</td>\n",
              "      <td>0.000282</td>\n",
              "      <td>0.000577</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>0.000642</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>0.000432</td>\n",
              "      <td>0.000287</td>\n",
              "      <td>0.000299</td>\n",
              "      <td>0.000276</td>\n",
              "      <td>0.000354</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000407</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>0.000322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>0.000247</td>\n",
              "      <td>0.000639</td>\n",
              "      <td>0.000750</td>\n",
              "      <td>0.000607</td>\n",
              "      <td>0.000413</td>\n",
              "      <td>0.000455</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>0.000476</td>\n",
              "      <td>0.000215</td>\n",
              "      <td>0.000332</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>0.000075</td>\n",
              "      <td>0.000713</td>\n",
              "      <td>0.000670</td>\n",
              "      <td>0.000759</td>\n",
              "      <td>0.000129</td>\n",
              "      <td>0.000507</td>\n",
              "      <td>0.000249</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000381</td>\n",
              "      <td>0.000492</td>\n",
              "      <td>0.000330</td>\n",
              "      <td>0.000602</td>\n",
              "      <td>0.000307</td>\n",
              "      <td>0.000326</td>\n",
              "      <td>0.000473</td>\n",
              "      <td>0.000214</td>\n",
              "      <td>0.000599</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>0.000101</td>\n",
              "      <td>0.000294</td>\n",
              "      <td>0.000410</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>0.000473</td>\n",
              "      <td>0.000715</td>\n",
              "      <td>0.000224</td>\n",
              "      <td>0.000292</td>\n",
              "      <td>0.000573</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000719</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000592</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>0.000393</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.000684</td>\n",
              "      <td>0.000655</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>0.000696</td>\n",
              "      <td>0.000621</td>\n",
              "      <td>0.000453</td>\n",
              "      <td>0.000460</td>\n",
              "      <td>0.000650</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>0.000463</td>\n",
              "      <td>0.000235</td>\n",
              "      <td>0.000728</td>\n",
              "      <td>0.000755</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.000593</td>\n",
              "      <td>0.000443</td>\n",
              "      <td>0.000707</td>\n",
              "      <td>0.000180</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000553</td>\n",
              "      <td>0.000144</td>\n",
              "      <td>0.000590</td>\n",
              "      <td>0.000765</td>\n",
              "      <td>0.000695</td>\n",
              "      <td>0.000673</td>\n",
              "      <td>0.000369</td>\n",
              "      <td>0.000169</td>\n",
              "      <td>0.000767</td>\n",
              "      <td>0.000226</td>\n",
              "      <td>0.000681</td>\n",
              "      <td>0.000572</td>\n",
              "      <td>0.000404</td>\n",
              "      <td>0.000455</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>0.000224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>0.000445</td>\n",
              "      <td>0.000434</td>\n",
              "      <td>0.000351</td>\n",
              "      <td>0.000071</td>\n",
              "      <td>0.000162</td>\n",
              "      <td>0.000322</td>\n",
              "      <td>0.000716</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.000136</td>\n",
              "      <td>0.000626</td>\n",
              "      <td>0.000145</td>\n",
              "      <td>0.000190</td>\n",
              "      <td>0.000314</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>0.000611</td>\n",
              "      <td>0.000716</td>\n",
              "      <td>0.000488</td>\n",
              "      <td>0.000380</td>\n",
              "      <td>0.000417</td>\n",
              "      <td>0.000628</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>0.000468</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.000487</td>\n",
              "      <td>0.000635</td>\n",
              "      <td>0.000478</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>0.000742</td>\n",
              "      <td>0.000575</td>\n",
              "      <td>0.000303</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>0.000665</td>\n",
              "      <td>0.000526</td>\n",
              "      <td>0.000744</td>\n",
              "      <td>0.000255</td>\n",
              "      <td>0.000447</td>\n",
              "      <td>0.000573</td>\n",
              "      <td>0.000280</td>\n",
              "      <td>0.000331</td>\n",
              "      <td>0.000655</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.000117</td>\n",
              "      <td>0.000618</td>\n",
              "      <td>0.000453</td>\n",
              "      <td>0.000277</td>\n",
              "      <td>0.000258</td>\n",
              "      <td>0.000502</td>\n",
              "      <td>0.000691</td>\n",
              "      <td>0.000543</td>\n",
              "      <td>0.000476</td>\n",
              "      <td>0.000725</td>\n",
              "      <td>0.000515</td>\n",
              "      <td>0.000546</td>\n",
              "      <td>0.000451</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.000556</td>\n",
              "      <td>0.000682</td>\n",
              "      <td>0.000575</td>\n",
              "      <td>0.000717</td>\n",
              "      <td>0.000527</td>\n",
              "      <td>0.000595</td>\n",
              "      <td>0.000663</td>\n",
              "      <td>0.000465</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>0.000391</td>\n",
              "      <td>0.000753</td>\n",
              "      <td>0.000284</td>\n",
              "      <td>0.000723</td>\n",
              "      <td>0.000753</td>\n",
              "      <td>0.000164</td>\n",
              "      <td>0.000492</td>\n",
              "      <td>0.000744</td>\n",
              "      <td>0.000462</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.000246</td>\n",
              "      <td>0.000277</td>\n",
              "      <td>0.000549</td>\n",
              "      <td>0.000587</td>\n",
              "      <td>0.000665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>678</th>\n",
              "      <td>0.000590</td>\n",
              "      <td>0.000346</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.000363</td>\n",
              "      <td>0.000178</td>\n",
              "      <td>0.000329</td>\n",
              "      <td>0.000518</td>\n",
              "      <td>0.000506</td>\n",
              "      <td>0.000486</td>\n",
              "      <td>0.000456</td>\n",
              "      <td>0.000633</td>\n",
              "      <td>0.000428</td>\n",
              "      <td>0.000669</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000475</td>\n",
              "      <td>0.000110</td>\n",
              "      <td>0.000393</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.000348</td>\n",
              "      <td>0.000385</td>\n",
              "      <td>0.000630</td>\n",
              "      <td>0.000217</td>\n",
              "      <td>0.000718</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.000430</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.000731</td>\n",
              "      <td>0.000597</td>\n",
              "      <td>0.000762</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>0.000472</td>\n",
              "      <td>0.000334</td>\n",
              "      <td>0.000047</td>\n",
              "      <td>0.000708</td>\n",
              "      <td>0.000725</td>\n",
              "      <td>0.000172</td>\n",
              "      <td>0.000650</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000326</td>\n",
              "      <td>0.000172</td>\n",
              "      <td>0.000450</td>\n",
              "      <td>0.000448</td>\n",
              "      <td>0.000495</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>0.000480</td>\n",
              "      <td>0.000593</td>\n",
              "      <td>0.000441</td>\n",
              "      <td>0.000312</td>\n",
              "      <td>0.000590</td>\n",
              "      <td>0.000763</td>\n",
              "      <td>0.000267</td>\n",
              "      <td>0.000185</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000171</td>\n",
              "      <td>0.000691</td>\n",
              "      <td>0.000762</td>\n",
              "      <td>0.000493</td>\n",
              "      <td>0.000734</td>\n",
              "      <td>0.000312</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.000287</td>\n",
              "      <td>0.000231</td>\n",
              "      <td>0.000726</td>\n",
              "      <td>0.000216</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>0.000480</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.000090</td>\n",
              "      <td>0.000326</td>\n",
              "      <td>0.000059</td>\n",
              "      <td>0.000754</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000132</td>\n",
              "      <td>0.000400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>402</th>\n",
              "      <td>0.000074</td>\n",
              "      <td>0.000576</td>\n",
              "      <td>0.000378</td>\n",
              "      <td>0.000463</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.000256</td>\n",
              "      <td>0.000237</td>\n",
              "      <td>0.000435</td>\n",
              "      <td>0.000181</td>\n",
              "      <td>0.000541</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.000451</td>\n",
              "      <td>0.000319</td>\n",
              "      <td>0.000730</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>0.000770</td>\n",
              "      <td>0.000522</td>\n",
              "      <td>0.000483</td>\n",
              "      <td>0.000291</td>\n",
              "      <td>0.000615</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>0.000353</td>\n",
              "      <td>0.000482</td>\n",
              "      <td>0.000542</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.000201</td>\n",
              "      <td>0.000550</td>\n",
              "      <td>0.000432</td>\n",
              "      <td>0.000240</td>\n",
              "      <td>0.000423</td>\n",
              "      <td>0.000457</td>\n",
              "      <td>0.000215</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000519</td>\n",
              "      <td>0.000740</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>0.000369</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.000325</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000249</td>\n",
              "      <td>0.000393</td>\n",
              "      <td>0.000395</td>\n",
              "      <td>0.000630</td>\n",
              "      <td>0.000342</td>\n",
              "      <td>0.000393</td>\n",
              "      <td>0.000539</td>\n",
              "      <td>0.000773</td>\n",
              "      <td>0.000601</td>\n",
              "      <td>0.000421</td>\n",
              "      <td>0.000603</td>\n",
              "      <td>0.000307</td>\n",
              "      <td>0.000713</td>\n",
              "      <td>0.000547</td>\n",
              "      <td>0.000606</td>\n",
              "      <td>0.000499</td>\n",
              "      <td>0.000666</td>\n",
              "      <td>0.000613</td>\n",
              "      <td>0.000119</td>\n",
              "      <td>0.000713</td>\n",
              "      <td>0.000618</td>\n",
              "      <td>0.000170</td>\n",
              "      <td>0.000152</td>\n",
              "      <td>0.000486</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000522</td>\n",
              "      <td>0.000638</td>\n",
              "      <td>0.000551</td>\n",
              "      <td>0.000460</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.000617</td>\n",
              "      <td>0.000558</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>0.000572</td>\n",
              "      <td>0.000277</td>\n",
              "      <td>0.000634</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000321</td>\n",
              "      <td>0.000686</td>\n",
              "      <td>0.000652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>927</th>\n",
              "      <td>0.000378</td>\n",
              "      <td>0.000403</td>\n",
              "      <td>0.000764</td>\n",
              "      <td>0.000633</td>\n",
              "      <td>0.000323</td>\n",
              "      <td>0.000410</td>\n",
              "      <td>0.000217</td>\n",
              "      <td>0.000642</td>\n",
              "      <td>0.000509</td>\n",
              "      <td>0.000438</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>0.000499</td>\n",
              "      <td>0.000309</td>\n",
              "      <td>0.000453</td>\n",
              "      <td>0.000352</td>\n",
              "      <td>0.000216</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.000430</td>\n",
              "      <td>0.000636</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.000283</td>\n",
              "      <td>0.000219</td>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.000628</td>\n",
              "      <td>0.000721</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.000469</td>\n",
              "      <td>0.000251</td>\n",
              "      <td>0.000665</td>\n",
              "      <td>0.000384</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>0.000260</td>\n",
              "      <td>0.000429</td>\n",
              "      <td>0.000458</td>\n",
              "      <td>0.000316</td>\n",
              "      <td>0.000375</td>\n",
              "      <td>0.000692</td>\n",
              "      <td>0.000640</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000596</td>\n",
              "      <td>0.000182</td>\n",
              "      <td>0.000741</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.000478</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000162</td>\n",
              "      <td>0.000432</td>\n",
              "      <td>0.000312</td>\n",
              "      <td>0.000495</td>\n",
              "      <td>0.000535</td>\n",
              "      <td>0.000270</td>\n",
              "      <td>0.000691</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>0.000458</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>0.000216</td>\n",
              "      <td>0.000332</td>\n",
              "      <td>0.000208</td>\n",
              "      <td>0.000384</td>\n",
              "      <td>0.000248</td>\n",
              "      <td>0.000639</td>\n",
              "      <td>0.000640</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>0.000460</td>\n",
              "      <td>0.000440</td>\n",
              "      <td>0.000593</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000628</td>\n",
              "      <td>0.000579</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>0.000614</td>\n",
              "      <td>0.000291</td>\n",
              "      <td>0.000512</td>\n",
              "      <td>0.000588</td>\n",
              "      <td>0.000536</td>\n",
              "      <td>0.000626</td>\n",
              "      <td>0.000187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>880</th>\n",
              "      <td>0.000269</td>\n",
              "      <td>0.000426</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.000314</td>\n",
              "      <td>0.000426</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.000441</td>\n",
              "      <td>0.000698</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.000695</td>\n",
              "      <td>0.000566</td>\n",
              "      <td>0.000702</td>\n",
              "      <td>0.000646</td>\n",
              "      <td>0.000128</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.000463</td>\n",
              "      <td>0.000702</td>\n",
              "      <td>0.000075</td>\n",
              "      <td>0.000313</td>\n",
              "      <td>0.000385</td>\n",
              "      <td>0.000295</td>\n",
              "      <td>0.000669</td>\n",
              "      <td>0.000328</td>\n",
              "      <td>0.000618</td>\n",
              "      <td>0.000614</td>\n",
              "      <td>0.000470</td>\n",
              "      <td>0.000594</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.000330</td>\n",
              "      <td>0.000406</td>\n",
              "      <td>0.000354</td>\n",
              "      <td>0.000676</td>\n",
              "      <td>0.000618</td>\n",
              "      <td>0.000305</td>\n",
              "      <td>0.000759</td>\n",
              "      <td>0.000475</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000557</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000215</td>\n",
              "      <td>0.000669</td>\n",
              "      <td>0.000475</td>\n",
              "      <td>0.000548</td>\n",
              "      <td>0.000385</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>0.000095</td>\n",
              "      <td>0.000414</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.000586</td>\n",
              "      <td>0.000409</td>\n",
              "      <td>0.000452</td>\n",
              "      <td>0.000762</td>\n",
              "      <td>0.000568</td>\n",
              "      <td>0.000082</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>0.000392</td>\n",
              "      <td>0.000716</td>\n",
              "      <td>0.000669</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.000398</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.000276</td>\n",
              "      <td>0.000594</td>\n",
              "      <td>0.000704</td>\n",
              "      <td>0.000307</td>\n",
              "      <td>0.000609</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.000392</td>\n",
              "      <td>0.000420</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>0.000569</td>\n",
              "      <td>0.000301</td>\n",
              "      <td>0.000730</td>\n",
              "      <td>0.000489</td>\n",
              "      <td>0.000663</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.000188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>679</th>\n",
              "      <td>0.000117</td>\n",
              "      <td>0.000557</td>\n",
              "      <td>0.000526</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000373</td>\n",
              "      <td>0.000346</td>\n",
              "      <td>0.000527</td>\n",
              "      <td>0.000309</td>\n",
              "      <td>0.000665</td>\n",
              "      <td>0.000385</td>\n",
              "      <td>0.000161</td>\n",
              "      <td>0.000493</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>0.000584</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>0.000654</td>\n",
              "      <td>0.000321</td>\n",
              "      <td>0.000446</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000540</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.000317</td>\n",
              "      <td>0.000736</td>\n",
              "      <td>0.000702</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>0.000543</td>\n",
              "      <td>0.000209</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.000532</td>\n",
              "      <td>0.000432</td>\n",
              "      <td>0.000276</td>\n",
              "      <td>0.000711</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000289</td>\n",
              "      <td>0.000296</td>\n",
              "      <td>0.000669</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000169</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000727</td>\n",
              "      <td>0.000348</td>\n",
              "      <td>0.000638</td>\n",
              "      <td>0.000605</td>\n",
              "      <td>0.000666</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.000580</td>\n",
              "      <td>0.000596</td>\n",
              "      <td>0.000687</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000485</td>\n",
              "      <td>0.000580</td>\n",
              "      <td>0.000495</td>\n",
              "      <td>0.000303</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000615</td>\n",
              "      <td>0.000743</td>\n",
              "      <td>0.000254</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.000613</td>\n",
              "      <td>0.000624</td>\n",
              "      <td>0.000538</td>\n",
              "      <td>0.000342</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.000350</td>\n",
              "      <td>0.000663</td>\n",
              "      <td>0.000652</td>\n",
              "      <td>0.000401</td>\n",
              "      <td>0.000286</td>\n",
              "      <td>0.000059</td>\n",
              "      <td>0.000679</td>\n",
              "      <td>0.000279</td>\n",
              "      <td>0.000495</td>\n",
              "      <td>0.000707</td>\n",
              "      <td>0.000758</td>\n",
              "      <td>0.000613</td>\n",
              "      <td>0.000630</td>\n",
              "      <td>0.000423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.000125</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>0.000310</td>\n",
              "      <td>0.000570</td>\n",
              "      <td>0.000595</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.000619</td>\n",
              "      <td>0.000524</td>\n",
              "      <td>0.000279</td>\n",
              "      <td>0.000376</td>\n",
              "      <td>0.000324</td>\n",
              "      <td>0.000760</td>\n",
              "      <td>0.000564</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>0.000168</td>\n",
              "      <td>0.000421</td>\n",
              "      <td>0.000447</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.000075</td>\n",
              "      <td>0.000220</td>\n",
              "      <td>0.000478</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>0.000391</td>\n",
              "      <td>0.000470</td>\n",
              "      <td>0.000750</td>\n",
              "      <td>0.000553</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.000293</td>\n",
              "      <td>0.000362</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>0.000082</td>\n",
              "      <td>0.000515</td>\n",
              "      <td>0.000385</td>\n",
              "      <td>0.000729</td>\n",
              "      <td>0.000661</td>\n",
              "      <td>0.000763</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000535</td>\n",
              "      <td>0.000729</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000511</td>\n",
              "      <td>0.000587</td>\n",
              "      <td>0.000752</td>\n",
              "      <td>0.000552</td>\n",
              "      <td>0.000140</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.000428</td>\n",
              "      <td>0.000479</td>\n",
              "      <td>0.000379</td>\n",
              "      <td>0.000522</td>\n",
              "      <td>0.000615</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.000404</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000542</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.000761</td>\n",
              "      <td>0.000627</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>0.000758</td>\n",
              "      <td>0.000659</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>0.000575</td>\n",
              "      <td>0.000522</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>0.000610</td>\n",
              "      <td>0.000622</td>\n",
              "      <td>0.000430</td>\n",
              "      <td>0.000481</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>0.000605</td>\n",
              "      <td>0.000402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>579</th>\n",
              "      <td>0.000666</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.000517</td>\n",
              "      <td>0.000671</td>\n",
              "      <td>0.000236</td>\n",
              "      <td>0.000560</td>\n",
              "      <td>0.000672</td>\n",
              "      <td>0.000605</td>\n",
              "      <td>0.000646</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.000703</td>\n",
              "      <td>0.000532</td>\n",
              "      <td>0.000228</td>\n",
              "      <td>0.000227</td>\n",
              "      <td>0.000763</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000120</td>\n",
              "      <td>0.000319</td>\n",
              "      <td>0.000666</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000332</td>\n",
              "      <td>0.000702</td>\n",
              "      <td>0.000507</td>\n",
              "      <td>0.000537</td>\n",
              "      <td>0.000275</td>\n",
              "      <td>0.000241</td>\n",
              "      <td>0.000327</td>\n",
              "      <td>0.000467</td>\n",
              "      <td>0.000319</td>\n",
              "      <td>0.000136</td>\n",
              "      <td>0.000333</td>\n",
              "      <td>0.000676</td>\n",
              "      <td>0.000267</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000430</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.000438</td>\n",
              "      <td>0.000253</td>\n",
              "      <td>0.000517</td>\n",
              "      <td>0.000634</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000316</td>\n",
              "      <td>0.000174</td>\n",
              "      <td>0.000379</td>\n",
              "      <td>0.000083</td>\n",
              "      <td>0.000625</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.000350</td>\n",
              "      <td>0.000544</td>\n",
              "      <td>0.000523</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>0.000185</td>\n",
              "      <td>0.000342</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.000268</td>\n",
              "      <td>0.000520</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>0.000643</td>\n",
              "      <td>0.000657</td>\n",
              "      <td>0.000720</td>\n",
              "      <td>0.000680</td>\n",
              "      <td>0.000762</td>\n",
              "      <td>0.000071</td>\n",
              "      <td>0.000660</td>\n",
              "      <td>0.000511</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>0.000507</td>\n",
              "      <td>0.000430</td>\n",
              "      <td>0.000335</td>\n",
              "      <td>0.000521</td>\n",
              "      <td>0.000503</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000697</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000723</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.000552</td>\n",
              "      <td>0.000350</td>\n",
              "      <td>0.000455</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>700 rows × 2500 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2     ...      2497      2498      2499\n",
              "851  0.000549  0.000737  0.000149  ...  0.000407  0.000368  0.000322\n",
              "190  0.000247  0.000639  0.000750  ...  0.000455  0.000155  0.000224\n",
              "798  0.000445  0.000434  0.000351  ...  0.000549  0.000587  0.000665\n",
              "678  0.000590  0.000346  0.000189  ...  0.000056  0.000132  0.000400\n",
              "402  0.000074  0.000576  0.000378  ...  0.000321  0.000686  0.000652\n",
              "..        ...       ...       ...  ...       ...       ...       ...\n",
              "927  0.000378  0.000403  0.000764  ...  0.000536  0.000626  0.000187\n",
              "880  0.000269  0.000426  0.000198  ...  0.000663  0.000072  0.000188\n",
              "679  0.000117  0.000557  0.000526  ...  0.000613  0.000630  0.000423\n",
              "18   0.000125  0.000176  0.000310  ...  0.000057  0.000605  0.000402\n",
              "579  0.000666  0.000242  0.000517  ...  0.000552  0.000350  0.000455\n",
              "\n",
              "[700 rows x 2500 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MKc7dcMed4p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "2735944a-28a8-4d3d-8928-5732ed3afd23"
      },
      "source": [
        "y_train_ann"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "865    1665\n",
              "905    1709\n",
              "805    1751\n",
              "812    1622\n",
              "632    1704\n",
              "       ... \n",
              "23     1835\n",
              "281    1802\n",
              "562    1656\n",
              "90     1746\n",
              "889    1750\n",
              "Name: val_data_ann, Length: 700, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VPQkKQ_eg0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Concatenating the split values for input and target respectively\n",
        "data_input_ann = np.concatenate((x_train_ann, x_test_ann), axis=0)\n",
        "data_target_ann = np.concatenate((y_train_ann, y_test_ann), axis=0)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C56Jf5nRNLF",
        "colab_type": "text"
      },
      "source": [
        "Using **MinMaxScaler()** to perform the scaling "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIBp_CAqexWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_scaling_ann = MinMaxScaler()\n",
        "scale_input_ann = data_scaling_ann.fit_transform(data_input_ann,2500)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-0eYrZJT9MW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "da57a03f-723e-4927-a5c9-d98ab50d8c84"
      },
      "source": [
        "scale_input_ann"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.68788332, 0.94505583, 0.15952821, ..., 0.5019797 , 0.45137507,\n",
              "        0.38799283],\n",
              "       [0.28763864, 0.81472453, 0.96482236, ..., 0.56544881, 0.16672525,\n",
              "        0.25828305],\n",
              "       [0.55000781, 0.54052228, 0.42964043, ..., 0.6905025 , 0.74596762,\n",
              "        0.84426179],\n",
              "       ...,\n",
              "       [0.90721354, 0.67266766, 0.47417641, ..., 0.47727134, 0.03694149,\n",
              "        0.97588978],\n",
              "       [0.7046278 , 0.24875103, 0.63726742, ..., 0.84876888, 0.3445137 ,\n",
              "        0.40697775],\n",
              "       [0.43933637, 0.36941367, 0.87218203, ..., 0.0498809 , 0.28476016,\n",
              "        0.96672716]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYDZchq8e5mJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "4f6ed449-98cf-4031-b1ac-900bc3972038"
      },
      "source": [
        "print(\"ANN Model\")\n",
        "print(\"Input Shape:\", scale_input_ann.shape)\n",
        "print(\"Traget Shape:\", data_target_ann.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ANN Model\n",
            "Input Shape: (1000, 2500)\n",
            "Traget Shape: (1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kIQq3VDRbYn",
        "colab_type": "text"
      },
      "source": [
        "Performing K Fold Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSXkhqPtfBVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialising k value as 7\n",
        "k_fold_val = 7\n",
        "k_fold_acc_ann = []\n",
        "k_fold_loss_ann = []"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhvSf6QsfE7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calling KFold function and initialising it to 'fk'\n",
        "fk = KFold(n_splits=k_fold_val, shuffle=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRY5_pWqRlpI",
        "colab_type": "text"
      },
      "source": [
        "Creating a ANN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvvY3X7kfJwE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "27ab3f42-10ae-4ce0-fed4-7af7c918067d"
      },
      "source": [
        "# Importing necessary tensor flow libraries for model compilation\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Training the ANN model for the respective k folds (7)\n",
        "for train, test in fk.split(scale_input_ann, data_target_ann):\n",
        "  model_ann = keras.Sequential()\n",
        "    \n",
        "  model_ann.add(layers.Dense(100, input_shape=(2500,), kernel_initializer='normal',bias_initializer='zeros', activation='relu', name = \"layer0\"))\n",
        "  model_ann.add(layers.Dense(75, kernel_initializer = 'normal', activation='relu', name = \"layer1\"))\n",
        "  model_ann.add(layers.Dense(50, kernel_initializer = 'normal', activation='relu', name = \"layer2\"))\n",
        "  model_ann.add(layers.Dense(25, kernel_initializer = 'normal', activation='relu', name = \"layer3\"))\n",
        "  model_ann.add(layers.Dense(10, kernel_initializer = 'normal', activation='relu', name = \"layer4\"))\n",
        "  model_ann.add(layers.Dense(5, kernel_initializer = 'normal', activation='relu', name = \"layer5\"))\n",
        "  model_ann.add(layers.Dense(1, kernel_initializer = 'normal', activation = \"linear\", name = \"layer6\"))\n",
        "  \n",
        "  model_ann.summary()\n",
        "  \n",
        "  # compiling model with RMSprop as the optimizer, mean square error as the loss function and mean absolute error as the metric\n",
        "  model_ann.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "  \n",
        "  history_ann = model_ann.fit(scale_input_ann[train], data_target_ann[train],\n",
        "                batch_size=1,epochs=50,verbose=1, validation_data = (scale_input_ann[test],data_target_ann[test]) )\n",
        "  \n",
        "  scores_ann = model_ann.evaluate(scale_input_ann[test], data_target_ann[test], verbose=0)\n",
        "  \n",
        "  model_ann.save('1106937_ANN.h5')\n",
        "  \n",
        "  k_fold_acc_ann.append(scores_ann[1] * 100)\n",
        "  k_fold_loss_ann.append(scores_ann[0])\n",
        "  print(scores_ann)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer0 (Dense)               (None, 100)               250100    \n",
            "_________________________________________________________________\n",
            "layer1 (Dense)               (None, 75)                7575      \n",
            "_________________________________________________________________\n",
            "layer2 (Dense)               (None, 50)                3800      \n",
            "_________________________________________________________________\n",
            "layer3 (Dense)               (None, 25)                1275      \n",
            "_________________________________________________________________\n",
            "layer4 (Dense)               (None, 10)                260       \n",
            "_________________________________________________________________\n",
            "layer5 (Dense)               (None, 5)                 55        \n",
            "_________________________________________________________________\n",
            "layer6 (Dense)               (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 263,071\n",
            "Trainable params: 263,071\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 264539.9062 - mae: 232.4270 - val_loss: 13005.2568 - val_mae: 95.2578\n",
            "Epoch 2/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 9917.9365 - mae: 80.3332 - val_loss: 7571.5063 - val_mae: 69.6211\n",
            "Epoch 3/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 8901.9160 - mae: 74.5918 - val_loss: 6334.9873 - val_mae: 62.6639\n",
            "Epoch 4/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 8543.7041 - mae: 74.0420 - val_loss: 6539.3003 - val_mae: 64.0543\n",
            "Epoch 5/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 7748.8716 - mae: 69.9813 - val_loss: 6889.6938 - val_mae: 65.5148\n",
            "Epoch 6/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 7336.6558 - mae: 68.2342 - val_loss: 8650.2139 - val_mae: 75.6532\n",
            "Epoch 7/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 6825.4487 - mae: 66.4194 - val_loss: 7389.3882 - val_mae: 68.3130\n",
            "Epoch 8/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 5339.2065 - mae: 57.9368 - val_loss: 7394.5762 - val_mae: 68.5701\n",
            "Epoch 9/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 4713.9634 - mae: 55.3441 - val_loss: 8039.9580 - val_mae: 71.3773\n",
            "Epoch 10/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 4084.8523 - mae: 51.7684 - val_loss: 7445.0254 - val_mae: 68.4637\n",
            "Epoch 11/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 3075.7393 - mae: 44.1541 - val_loss: 7739.8862 - val_mae: 69.9996\n",
            "Epoch 12/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 2531.5867 - mae: 41.1652 - val_loss: 17621.4414 - val_mae: 109.9427\n",
            "Epoch 13/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 2222.3247 - mae: 37.6435 - val_loss: 9032.9248 - val_mae: 76.3894\n",
            "Epoch 14/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1948.4749 - mae: 34.8377 - val_loss: 8703.2100 - val_mae: 74.0482\n",
            "Epoch 15/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1767.5815 - mae: 33.1614 - val_loss: 11743.2402 - val_mae: 87.9680\n",
            "Epoch 16/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1449.7462 - mae: 30.3905 - val_loss: 9350.5645 - val_mae: 77.2033\n",
            "Epoch 17/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1337.2986 - mae: 28.7184 - val_loss: 9562.8936 - val_mae: 77.2410\n",
            "Epoch 18/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1104.3879 - mae: 26.6259 - val_loss: 10098.7139 - val_mae: 81.0119\n",
            "Epoch 19/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1041.8877 - mae: 25.5585 - val_loss: 9523.3125 - val_mae: 78.1761\n",
            "Epoch 20/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 888.0820 - mae: 23.5674 - val_loss: 10213.6396 - val_mae: 81.6522\n",
            "Epoch 21/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 627.4851 - mae: 20.2694 - val_loss: 11291.5059 - val_mae: 85.6452\n",
            "Epoch 22/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 705.1343 - mae: 20.9114 - val_loss: 9791.1309 - val_mae: 79.3067\n",
            "Epoch 23/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 572.9839 - mae: 19.1997 - val_loss: 10749.7598 - val_mae: 81.9769\n",
            "Epoch 24/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 584.6055 - mae: 19.3661 - val_loss: 10211.6543 - val_mae: 80.4053\n",
            "Epoch 25/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 583.5659 - mae: 19.3586 - val_loss: 11220.2549 - val_mae: 83.8324\n",
            "Epoch 26/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 473.2984 - mae: 17.2808 - val_loss: 11189.3770 - val_mae: 83.5414\n",
            "Epoch 27/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 520.9355 - mae: 18.4184 - val_loss: 12686.6846 - val_mae: 89.3164\n",
            "Epoch 28/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 449.2946 - mae: 17.1575 - val_loss: 11229.3623 - val_mae: 83.5573\n",
            "Epoch 29/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 497.4099 - mae: 18.0354 - val_loss: 10946.8711 - val_mae: 82.7395\n",
            "Epoch 30/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 504.3949 - mae: 18.2739 - val_loss: 10785.4365 - val_mae: 83.4733\n",
            "Epoch 31/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 494.0381 - mae: 17.8973 - val_loss: 10597.5127 - val_mae: 82.2194\n",
            "Epoch 32/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 371.8409 - mae: 15.5912 - val_loss: 10633.7373 - val_mae: 83.2016\n",
            "Epoch 33/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 445.6649 - mae: 16.5131 - val_loss: 11126.7080 - val_mae: 83.8480\n",
            "Epoch 34/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 411.0111 - mae: 16.0781 - val_loss: 11105.4326 - val_mae: 85.2425\n",
            "Epoch 35/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 414.9979 - mae: 16.2311 - val_loss: 11455.7100 - val_mae: 84.7508\n",
            "Epoch 36/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 388.6871 - mae: 15.6978 - val_loss: 11039.5771 - val_mae: 84.4377\n",
            "Epoch 37/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 411.3822 - mae: 16.4777 - val_loss: 11216.2051 - val_mae: 85.3256\n",
            "Epoch 38/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 457.7052 - mae: 16.8014 - val_loss: 11024.2285 - val_mae: 84.9139\n",
            "Epoch 39/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 464.6175 - mae: 17.3507 - val_loss: 10943.7510 - val_mae: 84.7919\n",
            "Epoch 40/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 370.7991 - mae: 15.2228 - val_loss: 11132.9619 - val_mae: 85.2367\n",
            "Epoch 41/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 358.6847 - mae: 15.2615 - val_loss: 11146.0986 - val_mae: 85.5849\n",
            "Epoch 42/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 366.0968 - mae: 15.3408 - val_loss: 11038.1279 - val_mae: 84.6529\n",
            "Epoch 43/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 446.0702 - mae: 16.4328 - val_loss: 11865.6230 - val_mae: 86.1326\n",
            "Epoch 44/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 372.9023 - mae: 15.4409 - val_loss: 12698.2764 - val_mae: 91.0683\n",
            "Epoch 45/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 330.3809 - mae: 14.2645 - val_loss: 11014.1074 - val_mae: 84.6844\n",
            "Epoch 46/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 354.0585 - mae: 15.1862 - val_loss: 11394.4043 - val_mae: 86.5116\n",
            "Epoch 47/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 401.1100 - mae: 15.8532 - val_loss: 12389.7480 - val_mae: 89.9421\n",
            "Epoch 48/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 401.9822 - mae: 15.7104 - val_loss: 11220.1572 - val_mae: 86.0934\n",
            "Epoch 49/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 319.4954 - mae: 14.2135 - val_loss: 11010.7969 - val_mae: 83.6561\n",
            "Epoch 50/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 358.2930 - mae: 15.0124 - val_loss: 10979.4561 - val_mae: 83.8316\n",
            "[10979.4560546875, 83.83158874511719]\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer0 (Dense)               (None, 100)               250100    \n",
            "_________________________________________________________________\n",
            "layer1 (Dense)               (None, 75)                7575      \n",
            "_________________________________________________________________\n",
            "layer2 (Dense)               (None, 50)                3800      \n",
            "_________________________________________________________________\n",
            "layer3 (Dense)               (None, 25)                1275      \n",
            "_________________________________________________________________\n",
            "layer4 (Dense)               (None, 10)                260       \n",
            "_________________________________________________________________\n",
            "layer5 (Dense)               (None, 5)                 55        \n",
            "_________________________________________________________________\n",
            "layer6 (Dense)               (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 263,071\n",
            "Trainable params: 263,071\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 289931.8125 - mae: 243.1178 - val_loss: 15244.2451 - val_mae: 100.6775\n",
            "Epoch 2/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 9044.7295 - mae: 76.2603 - val_loss: 7191.9761 - val_mae: 70.4503\n",
            "Epoch 3/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 8886.5303 - mae: 75.5407 - val_loss: 7578.4946 - val_mae: 72.6042\n",
            "Epoch 4/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 8235.8965 - mae: 72.9440 - val_loss: 9492.6875 - val_mae: 77.5844\n",
            "Epoch 5/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 8099.3242 - mae: 72.3497 - val_loss: 8230.8330 - val_mae: 75.4341\n",
            "Epoch 6/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 7195.5566 - mae: 68.5892 - val_loss: 7181.3027 - val_mae: 70.4547\n",
            "Epoch 7/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 6497.4712 - mae: 64.8565 - val_loss: 7686.7036 - val_mae: 71.2903\n",
            "Epoch 8/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 5642.3477 - mae: 60.2870 - val_loss: 8242.2139 - val_mae: 72.8407\n",
            "Epoch 9/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 4898.9727 - mae: 54.8094 - val_loss: 7978.0840 - val_mae: 72.0996\n",
            "Epoch 10/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 4334.5938 - mae: 52.3674 - val_loss: 7411.9912 - val_mae: 70.9774\n",
            "Epoch 11/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 3870.8318 - mae: 49.6456 - val_loss: 7573.2026 - val_mae: 72.0169\n",
            "Epoch 12/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 3234.8472 - mae: 45.7214 - val_loss: 9023.4023 - val_mae: 79.5622\n",
            "Epoch 13/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 2654.1487 - mae: 41.2878 - val_loss: 9520.5723 - val_mae: 75.9132\n",
            "Epoch 14/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 2181.8452 - mae: 36.8008 - val_loss: 10252.1963 - val_mae: 78.0776\n",
            "Epoch 15/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1982.2046 - mae: 35.7641 - val_loss: 8929.5557 - val_mae: 78.4917\n",
            "Epoch 16/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1733.0436 - mae: 33.1587 - val_loss: 17245.7285 - val_mae: 106.0066\n",
            "Epoch 17/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1471.9211 - mae: 30.1200 - val_loss: 9070.2891 - val_mae: 78.2985\n",
            "Epoch 18/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1128.2118 - mae: 26.3836 - val_loss: 9454.0254 - val_mae: 79.8549\n",
            "Epoch 19/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1032.3777 - mae: 25.6854 - val_loss: 9549.9941 - val_mae: 79.6015\n",
            "Epoch 20/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 936.9667 - mae: 24.6251 - val_loss: 10883.6035 - val_mae: 85.7429\n",
            "Epoch 21/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 857.1868 - mae: 23.1444 - val_loss: 10207.5918 - val_mae: 79.5184\n",
            "Epoch 22/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 782.0988 - mae: 22.4311 - val_loss: 9254.0801 - val_mae: 77.0191\n",
            "Epoch 23/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 688.6155 - mae: 20.9032 - val_loss: 11720.2158 - val_mae: 84.7129\n",
            "Epoch 24/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 600.6943 - mae: 19.6692 - val_loss: 10592.2861 - val_mae: 80.5425\n",
            "Epoch 25/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 557.7018 - mae: 18.8519 - val_loss: 10662.7988 - val_mae: 81.0275\n",
            "Epoch 26/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 527.9975 - mae: 18.2396 - val_loss: 9692.4775 - val_mae: 77.9691\n",
            "Epoch 27/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 490.4576 - mae: 17.5107 - val_loss: 9749.3311 - val_mae: 78.1316\n",
            "Epoch 28/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 447.9362 - mae: 16.7239 - val_loss: 9797.5049 - val_mae: 78.4054\n",
            "Epoch 29/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 452.6654 - mae: 16.9853 - val_loss: 10144.9990 - val_mae: 80.3829\n",
            "Epoch 30/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 354.6041 - mae: 14.9129 - val_loss: 9927.8965 - val_mae: 79.5244\n",
            "Epoch 31/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 409.1500 - mae: 16.2864 - val_loss: 10794.8955 - val_mae: 81.8037\n",
            "Epoch 32/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 385.1562 - mae: 15.7713 - val_loss: 10307.1963 - val_mae: 80.3949\n",
            "Epoch 33/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 339.0578 - mae: 15.0508 - val_loss: 10655.6885 - val_mae: 81.9236\n",
            "Epoch 34/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 299.5891 - mae: 13.9752 - val_loss: 10675.8594 - val_mae: 81.8407\n",
            "Epoch 35/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 316.4952 - mae: 14.3157 - val_loss: 10181.9082 - val_mae: 80.1944\n",
            "Epoch 36/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 429.7396 - mae: 16.6823 - val_loss: 10111.7920 - val_mae: 80.8347\n",
            "Epoch 37/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 353.8557 - mae: 15.1731 - val_loss: 10532.4902 - val_mae: 81.1514\n",
            "Epoch 38/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 330.2021 - mae: 14.5128 - val_loss: 10037.0879 - val_mae: 79.9024\n",
            "Epoch 39/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 315.0248 - mae: 14.3759 - val_loss: 9992.7617 - val_mae: 80.2012\n",
            "Epoch 40/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 279.2987 - mae: 13.4283 - val_loss: 10015.2373 - val_mae: 80.2189\n",
            "Epoch 41/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 253.7221 - mae: 12.7371 - val_loss: 10310.4307 - val_mae: 80.8005\n",
            "Epoch 42/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 312.6240 - mae: 13.9443 - val_loss: 10532.0547 - val_mae: 82.1051\n",
            "Epoch 43/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 376.0563 - mae: 15.4469 - val_loss: 10913.5156 - val_mae: 82.5903\n",
            "Epoch 44/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 348.5419 - mae: 14.9575 - val_loss: 10128.1660 - val_mae: 80.0230\n",
            "Epoch 45/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 284.4740 - mae: 13.6375 - val_loss: 10638.1582 - val_mae: 81.6296\n",
            "Epoch 46/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 314.3658 - mae: 14.1370 - val_loss: 10061.6279 - val_mae: 79.9588\n",
            "Epoch 47/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 323.2955 - mae: 14.1791 - val_loss: 10185.6045 - val_mae: 80.7084\n",
            "Epoch 48/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 286.4619 - mae: 13.2478 - val_loss: 10568.0215 - val_mae: 81.6254\n",
            "Epoch 49/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 368.0475 - mae: 15.3478 - val_loss: 10110.6934 - val_mae: 80.3754\n",
            "Epoch 50/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 307.9872 - mae: 14.0261 - val_loss: 10125.0000 - val_mae: 80.3616\n",
            "[10125.005859375, 80.36162567138672]\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer0 (Dense)               (None, 100)               250100    \n",
            "_________________________________________________________________\n",
            "layer1 (Dense)               (None, 75)                7575      \n",
            "_________________________________________________________________\n",
            "layer2 (Dense)               (None, 50)                3800      \n",
            "_________________________________________________________________\n",
            "layer3 (Dense)               (None, 25)                1275      \n",
            "_________________________________________________________________\n",
            "layer4 (Dense)               (None, 10)                260       \n",
            "_________________________________________________________________\n",
            "layer5 (Dense)               (None, 5)                 55        \n",
            "_________________________________________________________________\n",
            "layer6 (Dense)               (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 263,071\n",
            "Trainable params: 263,071\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 272705.5312 - mae: 234.2200 - val_loss: 6981.0117 - val_mae: 69.0143\n",
            "Epoch 2/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 10328.5752 - mae: 81.1507 - val_loss: 11496.6240 - val_mae: 87.1042\n",
            "Epoch 3/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 9816.1162 - mae: 78.6426 - val_loss: 15477.4561 - val_mae: 102.2575\n",
            "Epoch 4/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 8830.2021 - mae: 75.5301 - val_loss: 27133.7246 - val_mae: 145.6126\n",
            "Epoch 5/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 7985.7427 - mae: 72.5265 - val_loss: 15924.8955 - val_mae: 105.1626\n",
            "Epoch 6/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 7145.2944 - mae: 67.0851 - val_loss: 7260.6543 - val_mae: 70.3588\n",
            "Epoch 7/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 6840.9785 - mae: 66.1863 - val_loss: 7278.0586 - val_mae: 69.4230\n",
            "Epoch 8/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 6052.3076 - mae: 61.9938 - val_loss: 7524.4688 - val_mae: 70.8892\n",
            "Epoch 9/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 4455.9458 - mae: 53.6958 - val_loss: 8422.8018 - val_mae: 75.1231\n",
            "Epoch 10/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 3567.0688 - mae: 47.8161 - val_loss: 8377.9961 - val_mae: 74.2821\n",
            "Epoch 11/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 3136.1094 - mae: 44.9232 - val_loss: 9039.6943 - val_mae: 77.0368\n",
            "Epoch 12/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 2487.5503 - mae: 40.1912 - val_loss: 15257.1123 - val_mae: 101.6181\n",
            "Epoch 13/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 2117.1218 - mae: 36.7393 - val_loss: 9821.8213 - val_mae: 79.7927\n",
            "Epoch 14/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1644.7206 - mae: 32.4876 - val_loss: 13335.4111 - val_mae: 93.1169\n",
            "Epoch 15/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1542.8292 - mae: 31.3036 - val_loss: 11945.6279 - val_mae: 87.4486\n",
            "Epoch 16/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1330.8934 - mae: 29.1568 - val_loss: 10508.2803 - val_mae: 82.8447\n",
            "Epoch 17/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1374.0593 - mae: 30.0016 - val_loss: 13450.2129 - val_mae: 93.1360\n",
            "Epoch 18/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1057.5563 - mae: 26.4098 - val_loss: 11444.6230 - val_mae: 86.2145\n",
            "Epoch 19/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1056.0117 - mae: 26.4158 - val_loss: 10761.0029 - val_mae: 83.8590\n",
            "Epoch 20/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 796.2534 - mae: 22.3377 - val_loss: 11213.1045 - val_mae: 84.8454\n",
            "Epoch 21/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 782.6472 - mae: 22.4540 - val_loss: 12052.0869 - val_mae: 88.2656\n",
            "Epoch 22/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 829.9340 - mae: 22.8880 - val_loss: 15899.5225 - val_mae: 102.5361\n",
            "Epoch 23/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 663.8052 - mae: 20.6708 - val_loss: 12195.5098 - val_mae: 89.2145\n",
            "Epoch 24/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 563.4118 - mae: 19.0258 - val_loss: 11723.7559 - val_mae: 87.6034\n",
            "Epoch 25/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 617.3248 - mae: 20.1907 - val_loss: 11694.2783 - val_mae: 87.1830\n",
            "Epoch 26/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 484.2375 - mae: 17.3993 - val_loss: 12316.4072 - val_mae: 89.7421\n",
            "Epoch 27/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 546.7207 - mae: 18.7077 - val_loss: 11385.7812 - val_mae: 86.1732\n",
            "Epoch 28/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 499.7567 - mae: 18.0397 - val_loss: 11893.5107 - val_mae: 88.1882\n",
            "Epoch 29/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 480.0987 - mae: 17.6701 - val_loss: 11527.8926 - val_mae: 87.0235\n",
            "Epoch 30/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 576.9537 - mae: 19.3192 - val_loss: 11842.6836 - val_mae: 87.5319\n",
            "Epoch 31/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 455.6599 - mae: 16.9666 - val_loss: 13351.9990 - val_mae: 93.9467\n",
            "Epoch 32/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 520.2436 - mae: 18.4298 - val_loss: 12388.0293 - val_mae: 90.3843\n",
            "Epoch 33/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 434.5666 - mae: 16.4702 - val_loss: 11836.0791 - val_mae: 88.2527\n",
            "Epoch 34/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 440.6515 - mae: 16.7730 - val_loss: 11686.2344 - val_mae: 86.9054\n",
            "Epoch 35/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 490.0167 - mae: 17.6692 - val_loss: 11832.3945 - val_mae: 87.9646\n",
            "Epoch 36/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 408.0008 - mae: 16.2143 - val_loss: 12600.0996 - val_mae: 91.1040\n",
            "Epoch 37/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 469.2760 - mae: 17.2664 - val_loss: 12279.9072 - val_mae: 89.9054\n",
            "Epoch 38/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 393.3861 - mae: 15.5126 - val_loss: 12577.5303 - val_mae: 91.3253\n",
            "Epoch 39/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 368.9404 - mae: 15.3756 - val_loss: 13259.8135 - val_mae: 94.1426\n",
            "Epoch 40/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 419.0879 - mae: 16.7970 - val_loss: 11702.8613 - val_mae: 87.8906\n",
            "Epoch 41/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 426.2120 - mae: 16.2662 - val_loss: 11644.0098 - val_mae: 87.7038\n",
            "Epoch 42/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 397.2255 - mae: 15.9334 - val_loss: 11787.8223 - val_mae: 88.0616\n",
            "Epoch 43/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 426.2567 - mae: 16.2873 - val_loss: 13071.4375 - val_mae: 92.9842\n",
            "Epoch 44/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 462.2442 - mae: 17.1097 - val_loss: 11761.9512 - val_mae: 87.5121\n",
            "Epoch 45/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 398.5870 - mae: 16.1985 - val_loss: 12178.3721 - val_mae: 89.3487\n",
            "Epoch 46/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 293.3961 - mae: 13.6950 - val_loss: 12405.0156 - val_mae: 89.6664\n",
            "Epoch 47/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 316.9728 - mae: 14.0433 - val_loss: 12390.4883 - val_mae: 90.3430\n",
            "Epoch 48/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 449.9759 - mae: 17.1856 - val_loss: 11765.0244 - val_mae: 88.0069\n",
            "Epoch 49/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 376.2651 - mae: 15.7878 - val_loss: 12400.5674 - val_mae: 90.8587\n",
            "Epoch 50/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 376.4316 - mae: 15.5510 - val_loss: 11821.4756 - val_mae: 88.6943\n",
            "[11821.4794921875, 88.69425964355469]\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer0 (Dense)               (None, 100)               250100    \n",
            "_________________________________________________________________\n",
            "layer1 (Dense)               (None, 75)                7575      \n",
            "_________________________________________________________________\n",
            "layer2 (Dense)               (None, 50)                3800      \n",
            "_________________________________________________________________\n",
            "layer3 (Dense)               (None, 25)                1275      \n",
            "_________________________________________________________________\n",
            "layer4 (Dense)               (None, 10)                260       \n",
            "_________________________________________________________________\n",
            "layer5 (Dense)               (None, 5)                 55        \n",
            "_________________________________________________________________\n",
            "layer6 (Dense)               (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 263,071\n",
            "Trainable params: 263,071\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 252374.7344 - mae: 219.6047 - val_loss: 12004.9756 - val_mae: 84.5293\n",
            "Epoch 2/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 10030.1738 - mae: 78.6019 - val_loss: 7876.4712 - val_mae: 72.7731\n",
            "Epoch 3/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 9097.1299 - mae: 76.5122 - val_loss: 20622.1758 - val_mae: 122.8990\n",
            "Epoch 4/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 7680.7251 - mae: 70.9697 - val_loss: 9217.7461 - val_mae: 81.9877\n",
            "Epoch 5/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 7058.4966 - mae: 67.0514 - val_loss: 12133.9512 - val_mae: 94.1436\n",
            "Epoch 6/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 6444.1426 - mae: 64.0909 - val_loss: 9105.7129 - val_mae: 81.4073\n",
            "Epoch 7/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 5731.3140 - mae: 61.7388 - val_loss: 9684.9785 - val_mae: 76.9731\n",
            "Epoch 8/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 5272.3701 - mae: 58.0453 - val_loss: 9399.7432 - val_mae: 82.3102\n",
            "Epoch 9/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 3856.5388 - mae: 48.8267 - val_loss: 12023.6416 - val_mae: 85.6531\n",
            "Epoch 10/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 3511.2261 - mae: 46.7420 - val_loss: 10069.0078 - val_mae: 84.6610\n",
            "Epoch 11/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 2874.8027 - mae: 42.9116 - val_loss: 10940.8379 - val_mae: 88.1175\n",
            "Epoch 12/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 2390.7781 - mae: 38.9804 - val_loss: 10336.5947 - val_mae: 82.9249\n",
            "Epoch 13/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 2107.0657 - mae: 36.5537 - val_loss: 10556.3301 - val_mae: 85.2347\n",
            "Epoch 14/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1464.6809 - mae: 30.5457 - val_loss: 11459.1396 - val_mae: 89.6178\n",
            "Epoch 15/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1429.2133 - mae: 30.3720 - val_loss: 11809.1416 - val_mae: 90.6434\n",
            "Epoch 16/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1407.5028 - mae: 29.9849 - val_loss: 14590.3379 - val_mae: 101.4583\n",
            "Epoch 17/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1078.4904 - mae: 26.3013 - val_loss: 11972.3330 - val_mae: 90.9718\n",
            "Epoch 18/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 930.8839 - mae: 24.1605 - val_loss: 13213.0576 - val_mae: 90.7406\n",
            "Epoch 19/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 885.6061 - mae: 23.7707 - val_loss: 12576.1602 - val_mae: 93.2108\n",
            "Epoch 20/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 768.6389 - mae: 22.3072 - val_loss: 12034.1211 - val_mae: 90.4040\n",
            "Epoch 21/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 813.6210 - mae: 22.8181 - val_loss: 12154.5293 - val_mae: 89.6263\n",
            "Epoch 22/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 899.6277 - mae: 24.1867 - val_loss: 12097.3711 - val_mae: 90.8716\n",
            "Epoch 23/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 743.7772 - mae: 21.9760 - val_loss: 12573.5156 - val_mae: 90.4317\n",
            "Epoch 24/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 514.4001 - mae: 18.3246 - val_loss: 14349.0137 - val_mae: 99.6795\n",
            "Epoch 25/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 448.9725 - mae: 16.6714 - val_loss: 16715.7051 - val_mae: 108.4421\n",
            "Epoch 26/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 456.4204 - mae: 17.2814 - val_loss: 12586.4697 - val_mae: 92.3330\n",
            "Epoch 27/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 639.4059 - mae: 20.0831 - val_loss: 12711.2041 - val_mae: 93.4088\n",
            "Epoch 28/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 518.9232 - mae: 18.3535 - val_loss: 12392.4561 - val_mae: 91.4611\n",
            "Epoch 29/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 458.9462 - mae: 16.9302 - val_loss: 13513.8809 - val_mae: 96.3016\n",
            "Epoch 30/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 429.6430 - mae: 16.5622 - val_loss: 13426.6699 - val_mae: 95.8388\n",
            "Epoch 31/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 430.8723 - mae: 16.7251 - val_loss: 13166.0156 - val_mae: 94.5439\n",
            "Epoch 32/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 421.2310 - mae: 16.5008 - val_loss: 15513.7764 - val_mae: 103.7967\n",
            "Epoch 33/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 442.0198 - mae: 16.6740 - val_loss: 14124.7812 - val_mae: 98.8223\n",
            "Epoch 34/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 394.4006 - mae: 15.8308 - val_loss: 12837.3857 - val_mae: 92.7894\n",
            "Epoch 35/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 470.8667 - mae: 16.8400 - val_loss: 12771.1709 - val_mae: 93.1304\n",
            "Epoch 36/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 431.2835 - mae: 16.4307 - val_loss: 12787.4209 - val_mae: 93.0500\n",
            "Epoch 37/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 381.7083 - mae: 15.5772 - val_loss: 13297.9912 - val_mae: 94.9737\n",
            "Epoch 38/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 472.7073 - mae: 17.3007 - val_loss: 13466.0449 - val_mae: 96.5710\n",
            "Epoch 39/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 529.5659 - mae: 18.1299 - val_loss: 12934.7168 - val_mae: 92.2661\n",
            "Epoch 40/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 475.5230 - mae: 17.4732 - val_loss: 12847.4160 - val_mae: 92.9117\n",
            "Epoch 41/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 472.4363 - mae: 17.4993 - val_loss: 12851.7783 - val_mae: 93.5747\n",
            "Epoch 42/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 430.5283 - mae: 16.5796 - val_loss: 12591.9863 - val_mae: 92.1952\n",
            "Epoch 43/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 359.7775 - mae: 15.0421 - val_loss: 14708.4443 - val_mae: 100.8379\n",
            "Epoch 44/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 452.8717 - mae: 17.0508 - val_loss: 14403.9658 - val_mae: 99.3159\n",
            "Epoch 45/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 479.1021 - mae: 17.4089 - val_loss: 12855.1553 - val_mae: 93.1508\n",
            "Epoch 46/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 468.1860 - mae: 17.1439 - val_loss: 13098.6338 - val_mae: 94.4486\n",
            "Epoch 47/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 378.3436 - mae: 15.3911 - val_loss: 12456.4795 - val_mae: 90.2626\n",
            "Epoch 48/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 346.7753 - mae: 14.7922 - val_loss: 12686.5625 - val_mae: 92.9964\n",
            "Epoch 49/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 305.4581 - mae: 14.0100 - val_loss: 13382.6543 - val_mae: 95.9111\n",
            "Epoch 50/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 374.9291 - mae: 15.4270 - val_loss: 12729.2129 - val_mae: 91.9649\n",
            "[12729.212890625, 91.96489715576172]\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer0 (Dense)               (None, 100)               250100    \n",
            "_________________________________________________________________\n",
            "layer1 (Dense)               (None, 75)                7575      \n",
            "_________________________________________________________________\n",
            "layer2 (Dense)               (None, 50)                3800      \n",
            "_________________________________________________________________\n",
            "layer3 (Dense)               (None, 25)                1275      \n",
            "_________________________________________________________________\n",
            "layer4 (Dense)               (None, 10)                260       \n",
            "_________________________________________________________________\n",
            "layer5 (Dense)               (None, 5)                 55        \n",
            "_________________________________________________________________\n",
            "layer6 (Dense)               (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 263,071\n",
            "Trainable params: 263,071\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 242147.9062 - mae: 221.2201 - val_loss: 8160.3784 - val_mae: 70.6096\n",
            "Epoch 2/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 10092.9629 - mae: 79.3720 - val_loss: 8020.1572 - val_mae: 73.8233\n",
            "Epoch 3/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 8870.5410 - mae: 76.7607 - val_loss: 17040.0000 - val_mae: 107.8061\n",
            "Epoch 4/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 8212.2666 - mae: 72.9268 - val_loss: 8451.2676 - val_mae: 72.0923\n",
            "Epoch 5/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 7617.6924 - mae: 70.7321 - val_loss: 7499.6416 - val_mae: 70.4355\n",
            "Epoch 6/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 7194.8784 - mae: 68.4121 - val_loss: 11594.9424 - val_mae: 85.0000\n",
            "Epoch 7/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 6417.9814 - mae: 64.2274 - val_loss: 7079.9233 - val_mae: 66.8856\n",
            "Epoch 8/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 5546.3545 - mae: 60.3530 - val_loss: 7107.8755 - val_mae: 66.3903\n",
            "Epoch 9/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 4421.1328 - mae: 53.2486 - val_loss: 10798.4375 - val_mae: 82.2229\n",
            "Epoch 10/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 4171.5273 - mae: 51.8516 - val_loss: 10223.8115 - val_mae: 83.0665\n",
            "Epoch 11/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 2996.6777 - mae: 43.7649 - val_loss: 7998.6934 - val_mae: 70.4693\n",
            "Epoch 12/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 3000.4045 - mae: 43.3711 - val_loss: 13625.2217 - val_mae: 93.1707\n",
            "Epoch 13/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 2136.3198 - mae: 36.1368 - val_loss: 8864.7627 - val_mae: 74.0790\n",
            "Epoch 14/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1980.6992 - mae: 34.6574 - val_loss: 9862.7939 - val_mae: 79.1988\n",
            "Epoch 15/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1652.4932 - mae: 32.4437 - val_loss: 9661.2910 - val_mae: 77.9268\n",
            "Epoch 16/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1398.1539 - mae: 30.0425 - val_loss: 9513.0908 - val_mae: 77.3977\n",
            "Epoch 17/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1386.0458 - mae: 29.1431 - val_loss: 9503.9756 - val_mae: 77.4423\n",
            "Epoch 18/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1077.0969 - mae: 25.8926 - val_loss: 9748.5732 - val_mae: 78.7496\n",
            "Epoch 19/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 938.4678 - mae: 24.3412 - val_loss: 10085.4121 - val_mae: 79.6896\n",
            "Epoch 20/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 816.7104 - mae: 22.4100 - val_loss: 10562.7861 - val_mae: 81.1927\n",
            "Epoch 21/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 716.6198 - mae: 21.2812 - val_loss: 10057.3301 - val_mae: 79.4133\n",
            "Epoch 22/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 750.6146 - mae: 22.0518 - val_loss: 10672.7129 - val_mae: 81.0312\n",
            "Epoch 23/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 847.5897 - mae: 22.6392 - val_loss: 10156.7666 - val_mae: 79.4668\n",
            "Epoch 24/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 692.5541 - mae: 20.9766 - val_loss: 10163.3262 - val_mae: 80.1892\n",
            "Epoch 25/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 521.3814 - mae: 18.1331 - val_loss: 10182.0898 - val_mae: 79.9403\n",
            "Epoch 26/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 468.7721 - mae: 17.2460 - val_loss: 10410.2041 - val_mae: 80.6916\n",
            "Epoch 27/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 412.0428 - mae: 16.1882 - val_loss: 10739.9160 - val_mae: 82.1279\n",
            "Epoch 28/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 570.1976 - mae: 19.2654 - val_loss: 11426.4180 - val_mae: 84.6439\n",
            "Epoch 29/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 525.8004 - mae: 18.1404 - val_loss: 11831.8389 - val_mae: 86.4316\n",
            "Epoch 30/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 475.8488 - mae: 17.2583 - val_loss: 11037.8203 - val_mae: 82.9545\n",
            "Epoch 31/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 429.2372 - mae: 16.5159 - val_loss: 10479.3379 - val_mae: 81.2856\n",
            "Epoch 32/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 496.7165 - mae: 17.7995 - val_loss: 11380.7002 - val_mae: 84.5913\n",
            "Epoch 33/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 476.4846 - mae: 17.5861 - val_loss: 10654.8086 - val_mae: 82.1939\n",
            "Epoch 34/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 390.2267 - mae: 16.0988 - val_loss: 11145.4072 - val_mae: 83.7500\n",
            "Epoch 35/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 403.6357 - mae: 15.8127 - val_loss: 10536.8184 - val_mae: 81.3652\n",
            "Epoch 36/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 374.0225 - mae: 15.4328 - val_loss: 10546.2607 - val_mae: 81.5897\n",
            "Epoch 37/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 502.0685 - mae: 17.9452 - val_loss: 10873.9971 - val_mae: 82.5520\n",
            "Epoch 38/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 505.7244 - mae: 18.3845 - val_loss: 11286.1445 - val_mae: 84.9701\n",
            "Epoch 39/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 471.6875 - mae: 17.4227 - val_loss: 10647.9102 - val_mae: 82.0695\n",
            "Epoch 40/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 493.1143 - mae: 17.7308 - val_loss: 10584.8945 - val_mae: 81.1111\n",
            "Epoch 41/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 569.5707 - mae: 18.5470 - val_loss: 10623.9980 - val_mae: 81.7420\n",
            "Epoch 42/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 370.3526 - mae: 15.1382 - val_loss: 11111.0068 - val_mae: 83.5510\n",
            "Epoch 43/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 416.1796 - mae: 16.2162 - val_loss: 11444.3574 - val_mae: 84.8630\n",
            "Epoch 44/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 417.9501 - mae: 16.3093 - val_loss: 10871.6611 - val_mae: 82.6590\n",
            "Epoch 45/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 439.0602 - mae: 16.8020 - val_loss: 11640.9238 - val_mae: 85.5767\n",
            "Epoch 46/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 428.0471 - mae: 16.9257 - val_loss: 10929.9668 - val_mae: 83.0262\n",
            "Epoch 47/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 386.2622 - mae: 15.6914 - val_loss: 10872.6982 - val_mae: 82.1556\n",
            "Epoch 48/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 375.3472 - mae: 15.4196 - val_loss: 11155.1953 - val_mae: 84.3411\n",
            "Epoch 49/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 550.0084 - mae: 19.0333 - val_loss: 11900.1025 - val_mae: 86.2253\n",
            "Epoch 50/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 407.0898 - mae: 16.0253 - val_loss: 11742.5254 - val_mae: 85.5274\n",
            "[11742.52734375, 85.52742767333984]\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer0 (Dense)               (None, 100)               250100    \n",
            "_________________________________________________________________\n",
            "layer1 (Dense)               (None, 75)                7575      \n",
            "_________________________________________________________________\n",
            "layer2 (Dense)               (None, 50)                3800      \n",
            "_________________________________________________________________\n",
            "layer3 (Dense)               (None, 25)                1275      \n",
            "_________________________________________________________________\n",
            "layer4 (Dense)               (None, 10)                260       \n",
            "_________________________________________________________________\n",
            "layer5 (Dense)               (None, 5)                 55        \n",
            "_________________________________________________________________\n",
            "layer6 (Dense)               (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 263,071\n",
            "Trainable params: 263,071\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 237874.0938 - mae: 215.8111 - val_loss: 14002.4941 - val_mae: 97.6634\n",
            "Epoch 2/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 9379.1543 - mae: 76.8098 - val_loss: 8188.2939 - val_mae: 71.7263\n",
            "Epoch 3/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 8653.1240 - mae: 74.8186 - val_loss: 10450.5596 - val_mae: 81.8419\n",
            "Epoch 4/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 8188.6982 - mae: 73.2394 - val_loss: 11558.2236 - val_mae: 86.8040\n",
            "Epoch 5/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 7078.1934 - mae: 67.4487 - val_loss: 11174.0381 - val_mae: 84.5328\n",
            "Epoch 6/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 6825.8115 - mae: 65.2401 - val_loss: 8580.0635 - val_mae: 73.6464\n",
            "Epoch 7/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 5771.7656 - mae: 62.1912 - val_loss: 8912.8428 - val_mae: 75.0733\n",
            "Epoch 8/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 4641.7505 - mae: 54.3412 - val_loss: 12642.2021 - val_mae: 90.3889\n",
            "Epoch 9/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 4161.1836 - mae: 52.2469 - val_loss: 11556.2578 - val_mae: 86.7178\n",
            "Epoch 10/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 3029.4792 - mae: 43.9503 - val_loss: 10261.4795 - val_mae: 80.9277\n",
            "Epoch 11/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 2897.3625 - mae: 43.2294 - val_loss: 10210.1543 - val_mae: 80.0739\n",
            "Epoch 12/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 2334.3013 - mae: 39.0160 - val_loss: 11182.1621 - val_mae: 84.6701\n",
            "Epoch 13/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1799.3950 - mae: 33.8384 - val_loss: 13883.4004 - val_mae: 94.9959\n",
            "Epoch 14/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1730.1375 - mae: 33.2930 - val_loss: 11626.8789 - val_mae: 84.9899\n",
            "Epoch 15/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1373.0366 - mae: 29.6319 - val_loss: 12809.1465 - val_mae: 90.9734\n",
            "Epoch 16/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1253.4310 - mae: 28.6336 - val_loss: 12221.4570 - val_mae: 87.0396\n",
            "Epoch 17/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1067.6372 - mae: 26.0181 - val_loss: 14783.1963 - val_mae: 96.6551\n",
            "Epoch 18/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1062.9160 - mae: 26.1380 - val_loss: 12934.8545 - val_mae: 92.1782\n",
            "Epoch 19/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 1018.5023 - mae: 25.6307 - val_loss: 12702.4766 - val_mae: 90.5755\n",
            "Epoch 20/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 738.6718 - mae: 21.8475 - val_loss: 12849.0537 - val_mae: 91.3229\n",
            "Epoch 21/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 603.5629 - mae: 19.5630 - val_loss: 12605.9463 - val_mae: 89.7791\n",
            "Epoch 22/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 527.1638 - mae: 18.5797 - val_loss: 12926.2539 - val_mae: 90.8099\n",
            "Epoch 23/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 628.5673 - mae: 19.9486 - val_loss: 12951.5371 - val_mae: 90.7667\n",
            "Epoch 24/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 632.5425 - mae: 20.2536 - val_loss: 14393.0859 - val_mae: 96.4235\n",
            "Epoch 25/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 514.3964 - mae: 17.9402 - val_loss: 14544.6250 - val_mae: 96.8327\n",
            "Epoch 26/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 509.9321 - mae: 18.1671 - val_loss: 13422.4512 - val_mae: 93.1892\n",
            "Epoch 27/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 511.4279 - mae: 17.8133 - val_loss: 13789.0811 - val_mae: 93.0821\n",
            "Epoch 28/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 601.9883 - mae: 19.4376 - val_loss: 13024.7090 - val_mae: 91.0938\n",
            "Epoch 29/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 648.7182 - mae: 20.4943 - val_loss: 13124.6152 - val_mae: 90.6854\n",
            "Epoch 30/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 507.1826 - mae: 17.8441 - val_loss: 13270.5713 - val_mae: 91.6375\n",
            "Epoch 31/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 505.1772 - mae: 18.0519 - val_loss: 13517.3848 - val_mae: 93.3173\n",
            "Epoch 32/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 464.2216 - mae: 17.1630 - val_loss: 13511.7998 - val_mae: 93.1869\n",
            "Epoch 33/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 443.7023 - mae: 16.9180 - val_loss: 13290.3525 - val_mae: 92.2284\n",
            "Epoch 34/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 464.0132 - mae: 17.0939 - val_loss: 13062.0039 - val_mae: 90.8170\n",
            "Epoch 35/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 477.3621 - mae: 17.3543 - val_loss: 13010.8936 - val_mae: 90.3971\n",
            "Epoch 36/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 454.6847 - mae: 17.0459 - val_loss: 13583.9082 - val_mae: 93.5067\n",
            "Epoch 37/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 445.7190 - mae: 16.8473 - val_loss: 12943.3340 - val_mae: 90.5537\n",
            "Epoch 38/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 381.5656 - mae: 15.6757 - val_loss: 13730.5713 - val_mae: 94.0483\n",
            "Epoch 39/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 486.7624 - mae: 18.2226 - val_loss: 13337.8818 - val_mae: 92.8341\n",
            "Epoch 40/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 418.0847 - mae: 16.3906 - val_loss: 13339.7822 - val_mae: 91.5041\n",
            "Epoch 41/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 451.8707 - mae: 17.0079 - val_loss: 13603.5479 - val_mae: 93.8062\n",
            "Epoch 42/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 547.7261 - mae: 18.3339 - val_loss: 14096.9082 - val_mae: 96.1295\n",
            "Epoch 43/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 512.1432 - mae: 18.3941 - val_loss: 13216.6885 - val_mae: 91.5502\n",
            "Epoch 44/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 417.5612 - mae: 16.5597 - val_loss: 13237.0928 - val_mae: 92.1608\n",
            "Epoch 45/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 371.6292 - mae: 15.3395 - val_loss: 14656.9443 - val_mae: 97.5755\n",
            "Epoch 46/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 420.4886 - mae: 16.1782 - val_loss: 13397.0410 - val_mae: 93.0865\n",
            "Epoch 47/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 381.7573 - mae: 15.4975 - val_loss: 13263.2285 - val_mae: 92.1571\n",
            "Epoch 48/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 317.8568 - mae: 14.3347 - val_loss: 13466.9658 - val_mae: 92.8036\n",
            "Epoch 49/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 308.9395 - mae: 14.0047 - val_loss: 13120.2793 - val_mae: 91.6068\n",
            "Epoch 50/50\n",
            "857/857 [==============================] - 2s 2ms/step - loss: 368.3132 - mae: 15.5644 - val_loss: 13183.9434 - val_mae: 91.9004\n",
            "[13183.9462890625, 91.9003677368164]\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer0 (Dense)               (None, 100)               250100    \n",
            "_________________________________________________________________\n",
            "layer1 (Dense)               (None, 75)                7575      \n",
            "_________________________________________________________________\n",
            "layer2 (Dense)               (None, 50)                3800      \n",
            "_________________________________________________________________\n",
            "layer3 (Dense)               (None, 25)                1275      \n",
            "_________________________________________________________________\n",
            "layer4 (Dense)               (None, 10)                260       \n",
            "_________________________________________________________________\n",
            "layer5 (Dense)               (None, 5)                 55        \n",
            "_________________________________________________________________\n",
            "layer6 (Dense)               (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 263,071\n",
            "Trainable params: 263,071\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 284749.7500 - mae: 242.7242 - val_loss: 13776.8125 - val_mae: 95.6474\n",
            "Epoch 2/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 9792.2666 - mae: 79.2941 - val_loss: 9604.2676 - val_mae: 78.0757\n",
            "Epoch 3/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 9828.5986 - mae: 79.5810 - val_loss: 10504.3730 - val_mae: 81.7057\n",
            "Epoch 4/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 8596.0918 - mae: 74.7755 - val_loss: 7385.2173 - val_mae: 68.6747\n",
            "Epoch 5/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 7263.7378 - mae: 68.7139 - val_loss: 9208.7852 - val_mae: 76.2734\n",
            "Epoch 6/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 7295.3535 - mae: 69.1366 - val_loss: 9792.8955 - val_mae: 81.4972\n",
            "Epoch 7/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 6629.0503 - mae: 64.6082 - val_loss: 15816.4033 - val_mae: 106.6516\n",
            "Epoch 8/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 5965.6035 - mae: 62.1114 - val_loss: 8814.5078 - val_mae: 75.1216\n",
            "Epoch 9/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 5414.3535 - mae: 59.3353 - val_loss: 15621.8730 - val_mae: 103.6011\n",
            "Epoch 10/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 4265.9658 - mae: 52.6800 - val_loss: 7894.4385 - val_mae: 70.7010\n",
            "Epoch 11/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 3761.7263 - mae: 49.0370 - val_loss: 8244.2549 - val_mae: 73.6980\n",
            "Epoch 12/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 3449.3884 - mae: 46.7239 - val_loss: 7206.2593 - val_mae: 67.5445\n",
            "Epoch 13/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 2438.5686 - mae: 39.7360 - val_loss: 8596.0098 - val_mae: 74.9942\n",
            "Epoch 14/50\n",
            "858/858 [==============================] - 4s 5ms/step - loss: 2110.7317 - mae: 36.6131 - val_loss: 8899.7441 - val_mae: 76.1141\n",
            "Epoch 15/50\n",
            "858/858 [==============================] - 4s 5ms/step - loss: 1705.2766 - mae: 33.3535 - val_loss: 7934.1885 - val_mae: 70.3839\n",
            "Epoch 16/50\n",
            "858/858 [==============================] - 3s 3ms/step - loss: 1523.4354 - mae: 31.7455 - val_loss: 9222.5918 - val_mae: 77.1614\n",
            "Epoch 17/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 1481.7245 - mae: 30.6582 - val_loss: 8733.7061 - val_mae: 74.2124\n",
            "Epoch 18/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 1119.9380 - mae: 26.6098 - val_loss: 11828.7305 - val_mae: 88.7044\n",
            "Epoch 19/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 1037.1384 - mae: 25.7746 - val_loss: 8966.5195 - val_mae: 74.7636\n",
            "Epoch 20/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 951.7427 - mae: 24.8252 - val_loss: 8595.9473 - val_mae: 72.3520\n",
            "Epoch 21/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 721.0500 - mae: 21.5576 - val_loss: 8738.4912 - val_mae: 72.7678\n",
            "Epoch 22/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 680.3739 - mae: 20.7394 - val_loss: 9040.9678 - val_mae: 74.2926\n",
            "Epoch 23/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 831.6158 - mae: 22.7968 - val_loss: 9088.5020 - val_mae: 74.2627\n",
            "Epoch 24/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 636.7239 - mae: 20.2389 - val_loss: 9923.6650 - val_mae: 78.7783\n",
            "Epoch 25/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 500.6627 - mae: 17.8974 - val_loss: 9208.9346 - val_mae: 74.9387\n",
            "Epoch 26/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 507.0282 - mae: 17.7739 - val_loss: 10633.4287 - val_mae: 81.2402\n",
            "Epoch 27/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 451.4055 - mae: 17.0800 - val_loss: 10660.0957 - val_mae: 81.0708\n",
            "Epoch 28/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 410.3507 - mae: 16.1720 - val_loss: 9287.5283 - val_mae: 75.8129\n",
            "Epoch 29/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 473.1082 - mae: 17.4342 - val_loss: 9420.9541 - val_mae: 76.6992\n",
            "Epoch 30/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 398.9446 - mae: 16.0388 - val_loss: 9351.4521 - val_mae: 75.9378\n",
            "Epoch 31/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 403.1637 - mae: 15.5226 - val_loss: 9476.8604 - val_mae: 76.8313\n",
            "Epoch 32/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 345.2849 - mae: 15.0156 - val_loss: 9528.1768 - val_mae: 77.1849\n",
            "Epoch 33/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 475.1243 - mae: 17.3795 - val_loss: 9736.7676 - val_mae: 77.8164\n",
            "Epoch 34/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 417.9731 - mae: 16.2272 - val_loss: 9811.7891 - val_mae: 77.6584\n",
            "Epoch 35/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 364.5948 - mae: 14.9283 - val_loss: 9623.0645 - val_mae: 77.1394\n",
            "Epoch 36/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 384.4518 - mae: 15.1130 - val_loss: 9688.2695 - val_mae: 77.3072\n",
            "Epoch 37/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 373.7459 - mae: 15.6442 - val_loss: 9796.5078 - val_mae: 77.3473\n",
            "Epoch 38/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 390.6207 - mae: 15.8056 - val_loss: 9632.2549 - val_mae: 77.1833\n",
            "Epoch 39/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 398.8208 - mae: 15.9743 - val_loss: 9841.9238 - val_mae: 77.9456\n",
            "Epoch 40/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 367.9232 - mae: 15.3310 - val_loss: 9679.2568 - val_mae: 77.7240\n",
            "Epoch 41/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 357.1103 - mae: 15.2209 - val_loss: 9707.5547 - val_mae: 77.1702\n",
            "Epoch 42/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 371.1852 - mae: 15.4307 - val_loss: 9842.1543 - val_mae: 77.8062\n",
            "Epoch 43/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 418.4801 - mae: 16.4391 - val_loss: 9683.6182 - val_mae: 77.4107\n",
            "Epoch 44/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 315.3185 - mae: 14.1554 - val_loss: 9674.9160 - val_mae: 77.6255\n",
            "Epoch 45/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 313.0299 - mae: 14.2642 - val_loss: 9617.6572 - val_mae: 77.1804\n",
            "Epoch 46/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 351.0628 - mae: 14.8986 - val_loss: 11039.8564 - val_mae: 82.4834\n",
            "Epoch 47/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 304.9329 - mae: 13.8630 - val_loss: 10221.1357 - val_mae: 78.8057\n",
            "Epoch 48/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 340.1731 - mae: 14.6884 - val_loss: 11015.6221 - val_mae: 83.1749\n",
            "Epoch 49/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 376.4162 - mae: 15.5080 - val_loss: 9697.1494 - val_mae: 77.2356\n",
            "Epoch 50/50\n",
            "858/858 [==============================] - 2s 2ms/step - loss: 392.9529 - mae: 15.9298 - val_loss: 9694.9424 - val_mae: 77.7959\n",
            "[9694.9453125, 77.79586791992188]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdvRtZZmgHvX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "95207b96-d853-47a5-9ec6-c91b61e233c2"
      },
      "source": [
        "#Printing the LOSS and Accuracy for ANN model\n",
        "\n",
        "print('------------------------------ANN-----------------------------------------')\n",
        "print('Score per fold for ann')\n",
        "for i in range(0, len(k_fold_acc_ann)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {k_fold_loss_ann[i]} - Accuracy: {k_fold_acc_ann[i]}')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds for ann:')\n",
        "print(f'> Accuracy: {np.mean(k_fold_acc_ann)} (+- {np.std(k_fold_acc_ann)})')\n",
        "print(f'> Loss: {np.mean(k_fold_loss_ann)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------ANN-----------------------------------------\n",
            "Score per fold for ann\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 10979.4560546875 - Accuracy: 8383.158874511719\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 10125.005859375 - Accuracy: 8036.162567138672\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 11821.4794921875 - Accuracy: 8869.425964355469\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 12729.212890625 - Accuracy: 9196.489715576172\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 11742.52734375 - Accuracy: 8552.742767333984\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 13183.9462890625 - Accuracy: 9190.03677368164\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 9694.9453125 - Accuracy: 7779.5867919921875\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds for ann:\n",
            "> Accuracy: 8572.51477922712 (+- 508.70837367630196)\n",
            "> Loss: 11468.08189174107\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84ODhr1Zbo2t",
        "colab_type": "text"
      },
      "source": [
        "Creating a graph for the loss in ANN model (epoch vs loss) involving training loss and validating loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBwyaAXZSn7_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "cf3b3ec0-1124-4933-9eb6-4dd90eb99ef8"
      },
      "source": [
        "plt.plot(history_ann.history['loss'])\n",
        "plt.plot(history_ann.history['val_loss'])\n",
        "plt.title('model loss ann')\n",
        "plt.ylabel('loss_ann')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()\n",
        "plt.savefig('loss_ann.png')\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dTNlPcbcHAO",
        "colab_type": "text"
      },
      "source": [
        "Loading and running the saved model for ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVoYhfAgS8bN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing the libraries to perform load and evaluate the saved model\n",
        "from keras.models import load_model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brUnAco1TBPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load model\n",
        "# model1 = load_model(\"1106937_ANN.h5\")\n",
        "modelnew_ann = tf.keras.models.load_model('1106937_ANN.h5')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyL88EEMWbin",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "d7529821-9e63-4b25-b97e-9c300deb6674"
      },
      "source": [
        "modelnew_ann.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer0 (Dense)               (None, 100)               250100    \n",
            "_________________________________________________________________\n",
            "layer1 (Dense)               (None, 75)                7575      \n",
            "_________________________________________________________________\n",
            "layer2 (Dense)               (None, 50)                3800      \n",
            "_________________________________________________________________\n",
            "layer3 (Dense)               (None, 25)                1275      \n",
            "_________________________________________________________________\n",
            "layer4 (Dense)               (None, 10)                260       \n",
            "_________________________________________________________________\n",
            "layer5 (Dense)               (None, 5)                 55        \n",
            "_________________________________________________________________\n",
            "layer6 (Dense)               (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 263,071\n",
            "Trainable params: 263,071\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmuhjjTrXuYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate the loaded model and calculating the scores\n",
        "scoresnew_ann = modelnew_ann.evaluate(scale_input_ann[test], data_target_ann[test], verbose=0)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7kJHDU0aJLu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b12149c3-ae71-4dfc-f071-69620fae97d6"
      },
      "source": [
        "# Printing the score\n",
        "print(scoresnew_ann)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9694.9453125, 77.79586791992188]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opaXskatZhLA",
        "colab_type": "text"
      },
      "source": [
        "Coding for the **CNN model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVjUon7sxoV0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6adb4e7b-448a-4f52-c387-d4f00b3db808"
      },
      "source": [
        "df1_cnn.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQRMedj7dZYY",
        "colab_type": "text"
      },
      "source": [
        "Reshaping the data matrix so that a CNN model can be built "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny2fvzWZg5kH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reshapecnn = []\n",
        "for i in range(1000):\n",
        "  reshape_2dcnn = np.reshape(df1_cnn.header_data_cnn[i], (50, 50))\n",
        "  reshapecnn.append(reshape_2dcnn)\n",
        "\n",
        "cnn_ip = np.asarray(reshapecnn)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqjyoddCyyfV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b2d36afb-5555-4a13-8c7f-4cdc4fe8d941"
      },
      "source": [
        "cnn_ip.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 50, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4rg7wLdiN5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a split for train and test of 70:30 ratio for the dataset\n",
        "x_train_cnn, x_test_cnn = train_test_split(cnn_ip, train_size = 0.7)\n",
        "y_train_cnn, y_test_cnn = train_test_split(df1_cnn.val_data_cnn, train_size = 0.7)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfd5E8gCxCFO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "dd2124f9-67d6-4a1b-aa06-755da7207790"
      },
      "source": [
        "# Scaling the dataset using MinMaxScaler\n",
        "data_scaling_cnn = MinMaxScaler()\n",
        "scale_input_cnn = data_scaling_cnn.fit_transform(l1_cnn,2500)\n",
        "scale_input_cnn"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.27753814, 0.98568715, 0.54455477, ..., 0.51442389, 0.58091678,\n",
              "        0.05583596],\n",
              "       [0.13465061, 0.05616399, 0.53276068, ..., 0.27997605, 1.        ,\n",
              "        0.42643034],\n",
              "       [0.09816313, 0.78265626, 0.28502689, ..., 0.43488515, 0.33188076,\n",
              "        0.78473294],\n",
              "       ...,\n",
              "       [0.66626438, 0.79522118, 0.19306748, ..., 0.2499735 , 0.61852049,\n",
              "        0.89682918],\n",
              "       [0.60979571, 0.97810167, 0.43778212, ..., 0.27984237, 0.61781701,\n",
              "        0.85678197],\n",
              "       [0.93058284, 0.31627963, 0.13169842, ..., 0.88304912, 0.14369493,\n",
              "        0.60778616]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iykmE5C5r74A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scale_input_cnn = np.concatenate((x_train_cnn, x_test_cnn), axis=0)\n",
        "data_target_cnn = np.concatenate((y_train_cnn, y_test_cnn), axis=0)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl_Iy5zNjwkK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "d726e730-0298-49c9-b9b4-5ba496c0079a"
      },
      "source": [
        "print(\"CNN Model\")\n",
        "print(\"Input Shape:\", scale_input_cnn.shape)\n",
        "print(\"Traget Shape:\", data_target_cnn.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN Model\n",
            "Input Shape: (1000, 50, 50)\n",
            "Traget Shape: (1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MabnReIkKTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k_fold_acc_cnn = []\n",
        "k_fold_loss_cnn = []"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXtlfDmLa4XK",
        "colab_type": "text"
      },
      "source": [
        "Importing the necessary libraries to create and compile the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSVWTJ8DvrSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling1D"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NRLaDuQkKoD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2b06eae7-4d58-4d5c-8203-c54b4a1747c4"
      },
      "source": [
        "# Training the CNN model for the respective k folds (9)\n",
        "for train, test in fk.split(scale_input_cnn, data_target_cnn):\n",
        "  model_cnn = keras.Sequential()\n",
        "    \n",
        "  model_cnn.add(layers.Conv1D(100,5, input_shape = (50,50), activation='relu', name = \"layer1\"))\n",
        "  model_cnn.add(layers.BatchNormalization(name = \"layer2\"))\n",
        "  model_cnn.add(layers.MaxPool1D(pool_size=5, name = \"layer3\"))\n",
        "  model_cnn.add(layers.Conv1D(75,5, activation='relu', name = \"layer4\"))\n",
        "  model_cnn.add(layers.BatchNormalization(name = \"layer5\"))\n",
        "  model_cnn.add(layers.Dropout(0.5, name = \"layer6\"))\n",
        "  model_cnn.add(layers.MaxPool1D(pool_size=5, name = \"layer7\"))\n",
        "  # model_cnn.add(layers.Conv1D(50,1, activation='relu', name = \"layer7\"))\n",
        "  # model_cnn.add(layers.BatchNormalization(name = \"layer8\"))\n",
        "  model_cnn.add(layers.Flatten(name = \"layer8\"))\n",
        "  model_cnn.add(layers.Dense(1, activation='linear', name = \"layer9\"))\n",
        "\n",
        "  model_cnn.summary()\n",
        "  \n",
        "  #compiling model with adam as the optimizer, mean square error as the loss function and mean absolute error as the metric\n",
        "  model_cnn.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "  \n",
        "  history_cnn = model_cnn.fit(scale_input_cnn[train], data_target_cnn[train],\n",
        "                batch_size=1,epochs=50,verbose=1, validation_data = (scale_input_cnn[test],data_target_cnn[test]))\n",
        "  \n",
        "  model_cnn.save('1106937_CNN.h5')\n",
        "  \n",
        "  scores_cnn = model_cnn.evaluate(scale_input_cnn[test], data_target_cnn[test], verbose=0)\n",
        "  \n",
        "  k_fold_acc_cnn.append(scores_cnn[1] * 100)\n",
        "  k_fold_loss_cnn.append(scores_cnn[0])\n",
        "  print(scores_cnn)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer1 (Conv1D)              (None, 46, 100)           25100     \n",
            "_________________________________________________________________\n",
            "layer2 (BatchNormalization)  (None, 46, 100)           400       \n",
            "_________________________________________________________________\n",
            "layer3 (MaxPooling1D)        (None, 9, 100)            0         \n",
            "_________________________________________________________________\n",
            "layer4 (Conv1D)              (None, 5, 75)             37575     \n",
            "_________________________________________________________________\n",
            "layer5 (BatchNormalization)  (None, 5, 75)             300       \n",
            "_________________________________________________________________\n",
            "layer6 (Dropout)             (None, 5, 75)             0         \n",
            "_________________________________________________________________\n",
            "layer7 (MaxPooling1D)        (None, 1, 75)             0         \n",
            "_________________________________________________________________\n",
            "layer8 (Flatten)             (None, 75)                0         \n",
            "_________________________________________________________________\n",
            "layer9 (Dense)               (None, 1)                 76        \n",
            "=================================================================\n",
            "Total params: 63,451\n",
            "Trainable params: 63,101\n",
            "Non-trainable params: 350\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 2720563.7500 - mae: 1645.1001 - val_loss: 2267103.2500 - val_mae: 1503.4263\n",
            "Epoch 2/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 1403015.5000 - mae: 1169.6722 - val_loss: 1681050.5000 - val_mae: 1290.7595\n",
            "Epoch 3/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 385644.1875 - mae: 584.8822 - val_loss: 750890.6250 - val_mae: 845.3953\n",
            "Epoch 4/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 64134.3047 - mae: 212.9237 - val_loss: 435593.3125 - val_mae: 627.2940\n",
            "Epoch 5/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 26395.7852 - mae: 128.2833 - val_loss: 452504.7812 - val_mae: 641.2235\n",
            "Epoch 6/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 22817.5703 - mae: 120.1153 - val_loss: 394171.2188 - val_mae: 596.8241\n",
            "Epoch 7/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 21585.9355 - mae: 118.0117 - val_loss: 482120.9062 - val_mae: 666.6889\n",
            "Epoch 8/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 22247.8281 - mae: 120.0021 - val_loss: 342463.8750 - val_mae: 552.8986\n",
            "Epoch 9/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 21946.8633 - mae: 117.6499 - val_loss: 227517.2031 - val_mae: 439.8361\n",
            "Epoch 10/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 24395.2344 - mae: 125.7417 - val_loss: 437906.0312 - val_mae: 636.1289\n",
            "Epoch 11/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 21700.9531 - mae: 116.5076 - val_loss: 341390.6562 - val_mae: 554.7946\n",
            "Epoch 12/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 20867.7930 - mae: 114.6080 - val_loss: 247892.0781 - val_mae: 463.2443\n",
            "Epoch 13/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 20269.7695 - mae: 112.3077 - val_loss: 263163.2188 - val_mae: 481.0669\n",
            "Epoch 14/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 22326.2832 - mae: 118.9697 - val_loss: 820725.4375 - val_mae: 890.4695\n",
            "Epoch 15/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 20605.7617 - mae: 115.2087 - val_loss: 662128.3750 - val_mae: 795.2394\n",
            "Epoch 16/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 20942.2637 - mae: 116.5162 - val_loss: 747702.1875 - val_mae: 846.9986\n",
            "Epoch 17/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 19880.2168 - mae: 112.4105 - val_loss: 49321.5352 - val_mae: 180.4033\n",
            "Epoch 18/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 21322.7812 - mae: 114.5721 - val_loss: 645992.8125 - val_mae: 787.3265\n",
            "Epoch 19/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 20643.2539 - mae: 116.0687 - val_loss: 61829.3711 - val_mae: 206.2433\n",
            "Epoch 20/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 19761.9062 - mae: 112.8597 - val_loss: 63459.7500 - val_mae: 209.0636\n",
            "Epoch 21/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 18403.5645 - mae: 106.9885 - val_loss: 708050.3125 - val_mae: 826.4190\n",
            "Epoch 22/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 19057.8887 - mae: 109.9811 - val_loss: 1528308.5000 - val_mae: 1226.7234\n",
            "Epoch 23/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 19753.0156 - mae: 111.8257 - val_loss: 265271.8750 - val_mae: 495.9083\n",
            "Epoch 24/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 16714.8066 - mae: 104.1411 - val_loss: 891566.0000 - val_mae: 931.2760\n",
            "Epoch 25/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 18569.0918 - mae: 111.2743 - val_loss: 463174.5312 - val_mae: 664.3411\n",
            "Epoch 26/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 18287.7148 - mae: 109.1037 - val_loss: 141059.7812 - val_mae: 346.7683\n",
            "Epoch 27/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 18240.0020 - mae: 108.8478 - val_loss: 24252.2207 - val_mae: 125.5678\n",
            "Epoch 28/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 17126.8848 - mae: 103.6090 - val_loss: 534005.0625 - val_mae: 718.0823\n",
            "Epoch 29/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 16876.7812 - mae: 103.1087 - val_loss: 160673.2031 - val_mae: 378.5029\n",
            "Epoch 30/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 18097.8125 - mae: 107.2134 - val_loss: 1135671.2500 - val_mae: 1057.6781\n",
            "Epoch 31/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 15439.4229 - mae: 100.5378 - val_loss: 74524.1094 - val_mae: 246.3582\n",
            "Epoch 32/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 15009.3496 - mae: 96.9764 - val_loss: 1836073.3750 - val_mae: 1349.3633\n",
            "Epoch 33/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 17020.7031 - mae: 104.9193 - val_loss: 338520.4062 - val_mae: 569.3685\n",
            "Epoch 34/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 15615.0801 - mae: 98.7511 - val_loss: 1551254.8750 - val_mae: 1239.7489\n",
            "Epoch 35/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 15484.5820 - mae: 99.4951 - val_loss: 799138.8750 - val_mae: 885.8246\n",
            "Epoch 36/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 14249.4268 - mae: 94.9561 - val_loss: 673484.1250 - val_mae: 812.6542\n",
            "Epoch 37/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 15009.7266 - mae: 96.6128 - val_loss: 132574.9688 - val_mae: 345.6613\n",
            "Epoch 38/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 14284.5430 - mae: 95.0093 - val_loss: 2792176.5000 - val_mae: 1666.1344\n",
            "Epoch 39/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 15242.1592 - mae: 98.3417 - val_loss: 1716994.8750 - val_mae: 1306.5115\n",
            "Epoch 40/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 14477.4150 - mae: 94.6906 - val_loss: 118584.3672 - val_mae: 327.1471\n",
            "Epoch 41/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 12671.6484 - mae: 89.9588 - val_loss: 3615288.2500 - val_mae: 1898.6497\n",
            "Epoch 42/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 13204.6006 - mae: 91.3790 - val_loss: 803870.3750 - val_mae: 890.4921\n",
            "Epoch 43/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 13931.0283 - mae: 94.3391 - val_loss: 44129.6523 - val_mae: 187.5558\n",
            "Epoch 44/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 12670.6816 - mae: 88.5080 - val_loss: 71271.9922 - val_mae: 247.9323\n",
            "Epoch 45/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 13684.5234 - mae: 92.7577 - val_loss: 31486.1504 - val_mae: 153.1752\n",
            "Epoch 46/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 12300.8369 - mae: 88.3073 - val_loss: 10872.7207 - val_mae: 83.3529\n",
            "Epoch 47/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 11386.5674 - mae: 84.5349 - val_loss: 13218.8145 - val_mae: 93.2511\n",
            "Epoch 48/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 13205.7881 - mae: 90.1911 - val_loss: 704498.9375 - val_mae: 833.9439\n",
            "Epoch 49/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 11914.6279 - mae: 86.7628 - val_loss: 1494832.5000 - val_mae: 1218.8052\n",
            "Epoch 50/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 10708.0254 - mae: 82.0511 - val_loss: 88762.3672 - val_mae: 284.2127\n",
            "[88762.390625, 284.2126770019531]\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer1 (Conv1D)              (None, 46, 100)           25100     \n",
            "_________________________________________________________________\n",
            "layer2 (BatchNormalization)  (None, 46, 100)           400       \n",
            "_________________________________________________________________\n",
            "layer3 (MaxPooling1D)        (None, 9, 100)            0         \n",
            "_________________________________________________________________\n",
            "layer4 (Conv1D)              (None, 5, 75)             37575     \n",
            "_________________________________________________________________\n",
            "layer5 (BatchNormalization)  (None, 5, 75)             300       \n",
            "_________________________________________________________________\n",
            "layer6 (Dropout)             (None, 5, 75)             0         \n",
            "_________________________________________________________________\n",
            "layer7 (MaxPooling1D)        (None, 1, 75)             0         \n",
            "_________________________________________________________________\n",
            "layer8 (Flatten)             (None, 75)                0         \n",
            "_________________________________________________________________\n",
            "layer9 (Dense)               (None, 1)                 76        \n",
            "=================================================================\n",
            "Total params: 63,451\n",
            "Trainable params: 63,101\n",
            "Non-trainable params: 350\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 2708472.5000 - mae: 1641.4170 - val_loss: 2086780.3750 - val_mae: 1441.6266\n",
            "Epoch 2/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 1338384.2500 - mae: 1139.4464 - val_loss: 1689377.0000 - val_mae: 1294.0742\n",
            "Epoch 3/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 332000.9375 - mae: 541.1112 - val_loss: 715437.6875 - val_mae: 827.9422\n",
            "Epoch 4/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 45563.6562 - mae: 174.7681 - val_loss: 357134.5312 - val_mae: 568.5263\n",
            "Epoch 5/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 22317.8164 - mae: 119.9499 - val_loss: 403828.4062 - val_mae: 609.6897\n",
            "Epoch 6/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 23887.7578 - mae: 123.7928 - val_loss: 446519.1562 - val_mae: 643.3528\n",
            "Epoch 7/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 22925.7871 - mae: 120.1681 - val_loss: 479703.0625 - val_mae: 669.2277\n",
            "Epoch 8/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 20860.3613 - mae: 114.7393 - val_loss: 480561.2188 - val_mae: 670.9069\n",
            "Epoch 9/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 22460.3203 - mae: 118.7164 - val_loss: 374467.2812 - val_mae: 587.6310\n",
            "Epoch 10/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 20417.1758 - mae: 113.5091 - val_loss: 405814.5312 - val_mae: 613.7072\n",
            "Epoch 11/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 21645.5684 - mae: 118.3598 - val_loss: 468001.6250 - val_mae: 664.3181\n",
            "Epoch 12/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 21445.7344 - mae: 119.6657 - val_loss: 278507.6250 - val_mae: 501.4882\n",
            "Epoch 13/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 20238.1934 - mae: 114.7361 - val_loss: 359883.5625 - val_mae: 576.7994\n",
            "Epoch 14/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 21806.0879 - mae: 119.4827 - val_loss: 576479.5000 - val_mae: 741.1652\n",
            "Epoch 15/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 19656.2598 - mae: 113.4908 - val_loss: 412785.5000 - val_mae: 621.1674\n",
            "Epoch 16/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 18258.1660 - mae: 108.0094 - val_loss: 475992.5000 - val_mae: 670.2598\n",
            "Epoch 17/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 19307.6855 - mae: 110.6952 - val_loss: 191022.9062 - val_mae: 407.3566\n",
            "Epoch 18/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 20021.2949 - mae: 113.8647 - val_loss: 1362789.6250 - val_mae: 1155.5657\n",
            "Epoch 19/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 19119.8633 - mae: 110.9482 - val_loss: 957117.7500 - val_mae: 965.4548\n",
            "Epoch 20/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 18250.5820 - mae: 106.7835 - val_loss: 207185.0000 - val_mae: 426.8428\n",
            "Epoch 21/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 17684.5234 - mae: 105.3168 - val_loss: 3278123.7500 - val_mae: 1804.9054\n",
            "Epoch 22/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 18460.0645 - mae: 109.9742 - val_loss: 652297.1875 - val_mae: 792.6122\n",
            "Epoch 23/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 19315.1641 - mae: 110.2715 - val_loss: 1158259.5000 - val_mae: 1066.5922\n",
            "Epoch 24/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 18640.5488 - mae: 109.4998 - val_loss: 248497.4844 - val_mae: 477.6680\n",
            "Epoch 25/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 18603.7598 - mae: 108.4722 - val_loss: 19355.1055 - val_mae: 115.8561\n",
            "Epoch 26/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 17449.0625 - mae: 106.1614 - val_loss: 1107433.0000 - val_mae: 1043.7670\n",
            "Epoch 27/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 15733.1396 - mae: 99.0438 - val_loss: 24231.6172 - val_mae: 130.8627\n",
            "Epoch 28/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 17453.6250 - mae: 104.6635 - val_loss: 1172799.2500 - val_mae: 1074.5013\n",
            "Epoch 29/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 16208.0742 - mae: 101.8982 - val_loss: 678177.1875 - val_mae: 812.9886\n",
            "Epoch 30/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 15818.7061 - mae: 100.8009 - val_loss: 1398110.6250 - val_mae: 1176.8611\n",
            "Epoch 31/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 14979.5293 - mae: 96.5634 - val_loss: 968159.1250 - val_mae: 975.2335\n",
            "Epoch 32/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 15164.9697 - mae: 98.9215 - val_loss: 917661.0000 - val_mae: 949.9005\n",
            "Epoch 33/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 15062.4014 - mae: 96.7272 - val_loss: 164180.0938 - val_mae: 383.7517\n",
            "Epoch 34/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 15524.1621 - mae: 99.7777 - val_loss: 261093.4844 - val_mae: 496.0772\n",
            "Epoch 35/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 13675.2441 - mae: 92.9909 - val_loss: 4125633.7500 - val_mae: 2027.9161\n",
            "Epoch 36/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 15345.8555 - mae: 97.1256 - val_loss: 1878526.6250 - val_mae: 1365.6747\n",
            "Epoch 37/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 13349.6348 - mae: 91.4805 - val_loss: 46446.5234 - val_mae: 185.3269\n",
            "Epoch 38/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 14141.4180 - mae: 95.3540 - val_loss: 2924362.0000 - val_mae: 1705.8248\n",
            "Epoch 39/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 13098.5986 - mae: 90.5143 - val_loss: 549026.5000 - val_mae: 731.2675\n",
            "Epoch 40/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 12627.8799 - mae: 89.6802 - val_loss: 2414488.5000 - val_mae: 1549.2161\n",
            "Epoch 41/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 12588.1621 - mae: 90.0539 - val_loss: 1406832.7500 - val_mae: 1180.2227\n",
            "Epoch 42/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 12246.7764 - mae: 89.5300 - val_loss: 104525.5469 - val_mae: 301.9199\n",
            "Epoch 43/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 11690.5557 - mae: 86.5331 - val_loss: 2571570.0000 - val_mae: 1599.7571\n",
            "Epoch 44/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 11538.7051 - mae: 86.0556 - val_loss: 3703778.2500 - val_mae: 1921.4760\n",
            "Epoch 45/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 11924.2041 - mae: 87.2524 - val_loss: 453165.9062 - val_mae: 664.4500\n",
            "Epoch 46/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 12204.4531 - mae: 88.1064 - val_loss: 192413.5781 - val_mae: 425.6376\n",
            "Epoch 47/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 10298.5107 - mae: 80.9207 - val_loss: 463036.4062 - val_mae: 672.3245\n",
            "Epoch 48/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 10603.3096 - mae: 83.2376 - val_loss: 81731.1172 - val_mae: 267.2938\n",
            "Epoch 49/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 10987.3848 - mae: 83.5896 - val_loss: 3193859.2500 - val_mae: 1784.2426\n",
            "Epoch 50/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 9954.0225 - mae: 79.1236 - val_loss: 30553.3711 - val_mae: 149.2949\n",
            "[30553.345703125, 149.2947998046875]\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer1 (Conv1D)              (None, 46, 100)           25100     \n",
            "_________________________________________________________________\n",
            "layer2 (BatchNormalization)  (None, 46, 100)           400       \n",
            "_________________________________________________________________\n",
            "layer3 (MaxPooling1D)        (None, 9, 100)            0         \n",
            "_________________________________________________________________\n",
            "layer4 (Conv1D)              (None, 5, 75)             37575     \n",
            "_________________________________________________________________\n",
            "layer5 (BatchNormalization)  (None, 5, 75)             300       \n",
            "_________________________________________________________________\n",
            "layer6 (Dropout)             (None, 5, 75)             0         \n",
            "_________________________________________________________________\n",
            "layer7 (MaxPooling1D)        (None, 1, 75)             0         \n",
            "_________________________________________________________________\n",
            "layer8 (Flatten)             (None, 75)                0         \n",
            "_________________________________________________________________\n",
            "layer9 (Dense)               (None, 1)                 76        \n",
            "=================================================================\n",
            "Total params: 63,451\n",
            "Trainable params: 63,101\n",
            "Non-trainable params: 350\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 2699363.5000 - mae: 1638.4730 - val_loss: 1977631.1250 - val_mae: 1403.6733\n",
            "Epoch 2/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 1335572.7500 - mae: 1139.2675 - val_loss: 1447203.6250 - val_mae: 1193.2507\n",
            "Epoch 3/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 336099.0312 - mae: 544.9260 - val_loss: 625561.6875 - val_mae: 756.8793\n",
            "Epoch 4/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 49237.7734 - mae: 184.3988 - val_loss: 534809.3750 - val_mae: 691.0599\n",
            "Epoch 5/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 25293.7988 - mae: 126.3588 - val_loss: 396709.4375 - val_mae: 588.6641\n",
            "Epoch 6/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 24271.4336 - mae: 124.9518 - val_loss: 476416.1250 - val_mae: 653.4512\n",
            "Epoch 7/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 22330.7500 - mae: 118.3307 - val_loss: 390878.8438 - val_mae: 585.9398\n",
            "Epoch 8/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 21989.5488 - mae: 116.3631 - val_loss: 451768.0312 - val_mae: 633.3545\n",
            "Epoch 9/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 21438.0449 - mae: 117.7989 - val_loss: 390715.6250 - val_mae: 589.5302\n",
            "Epoch 10/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 22279.1836 - mae: 119.5103 - val_loss: 477647.7188 - val_mae: 658.4662\n",
            "Epoch 11/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 22773.1738 - mae: 122.2425 - val_loss: 241075.9219 - val_mae: 453.3025\n",
            "Epoch 12/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 19440.7344 - mae: 112.7051 - val_loss: 240854.6250 - val_mae: 454.9781\n",
            "Epoch 13/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 18868.7832 - mae: 110.5354 - val_loss: 322640.1250 - val_mae: 534.5280\n",
            "Epoch 14/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 21119.1641 - mae: 116.8753 - val_loss: 798734.0625 - val_mae: 872.8837\n",
            "Epoch 15/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 19914.5352 - mae: 112.0097 - val_loss: 136181.9375 - val_mae: 327.9857\n",
            "Epoch 16/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 20172.6699 - mae: 115.0906 - val_loss: 1401647.8750 - val_mae: 1168.2643\n",
            "Epoch 17/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 20685.2539 - mae: 114.7447 - val_loss: 682844.0000 - val_mae: 804.9487\n",
            "Epoch 18/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 19555.6152 - mae: 109.3677 - val_loss: 361303.7188 - val_mae: 573.4009\n",
            "Epoch 19/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 17982.2266 - mae: 105.9306 - val_loss: 645210.3125 - val_mae: 784.9820\n",
            "Epoch 20/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 20458.7207 - mae: 113.5496 - val_loss: 364106.0312 - val_mae: 578.5019\n",
            "Epoch 21/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 18309.0332 - mae: 106.5648 - val_loss: 68841.7031 - val_mae: 213.6581\n",
            "Epoch 22/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 18556.2363 - mae: 108.6880 - val_loss: 38233.0508 - val_mae: 163.3470\n",
            "Epoch 23/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 16427.1836 - mae: 102.9822 - val_loss: 282577.0312 - val_mae: 504.0732\n",
            "Epoch 24/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 17060.4160 - mae: 103.9769 - val_loss: 33157.4648 - val_mae: 151.1188\n",
            "Epoch 25/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 17647.0840 - mae: 107.3054 - val_loss: 469024.8125 - val_mae: 666.1711\n",
            "Epoch 26/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 18364.5684 - mae: 108.4871 - val_loss: 729356.1875 - val_mae: 839.1045\n",
            "Epoch 27/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 16991.8398 - mae: 103.9686 - val_loss: 1210575.2500 - val_mae: 1090.5317\n",
            "Epoch 28/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 17098.1602 - mae: 102.5476 - val_loss: 4312826.5000 - val_mae: 2071.2117\n",
            "Epoch 29/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 16355.4424 - mae: 101.4202 - val_loss: 1638844.7500 - val_mae: 1272.9600\n",
            "Epoch 30/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 14845.9053 - mae: 96.6401 - val_loss: 22301.9570 - val_mae: 118.4632\n",
            "Epoch 31/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 15897.0166 - mae: 100.0624 - val_loss: 635920.7500 - val_mae: 784.4362\n",
            "Epoch 32/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 15521.8760 - mae: 99.5646 - val_loss: 335778.5312 - val_mae: 563.8050\n",
            "Epoch 33/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 14513.5146 - mae: 96.8044 - val_loss: 1992107.3750 - val_mae: 1405.2040\n",
            "Epoch 34/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 14536.4834 - mae: 95.5226 - val_loss: 97334.4844 - val_mae: 284.8001\n",
            "Epoch 35/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 14602.5908 - mae: 96.4072 - val_loss: 1499818.3750 - val_mae: 1218.3320\n",
            "Epoch 36/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 14604.2100 - mae: 96.7370 - val_loss: 1737443.6250 - val_mae: 1312.3938\n",
            "Epoch 37/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 14454.2334 - mae: 96.4216 - val_loss: 97598.3281 - val_mae: 286.0427\n",
            "Epoch 38/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 14153.3184 - mae: 94.5452 - val_loss: 758044.2500 - val_mae: 862.0618\n",
            "Epoch 39/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 13338.2510 - mae: 92.7739 - val_loss: 462219.5938 - val_mae: 669.2633\n",
            "Epoch 40/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 14836.7295 - mae: 98.4309 - val_loss: 35676.3398 - val_mae: 162.5993\n",
            "Epoch 41/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 12778.5215 - mae: 90.7643 - val_loss: 263413.4375 - val_mae: 500.1859\n",
            "Epoch 42/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 12575.4062 - mae: 89.6478 - val_loss: 109094.5781 - val_mae: 310.8472\n",
            "Epoch 43/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 12585.8379 - mae: 89.4651 - val_loss: 2460989.0000 - val_mae: 1565.1796\n",
            "Epoch 44/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 12697.8369 - mae: 90.0397 - val_loss: 11328.4961 - val_mae: 86.1944\n",
            "Epoch 45/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 12345.0381 - mae: 87.6119 - val_loss: 61825.5508 - val_mae: 226.2083\n",
            "Epoch 46/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 12053.0117 - mae: 87.7034 - val_loss: 735297.5625 - val_mae: 851.3300\n",
            "Epoch 47/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 11525.6035 - mae: 85.5412 - val_loss: 32488.2695 - val_mae: 154.5361\n",
            "Epoch 48/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 11141.7256 - mae: 83.2607 - val_loss: 314056.3438 - val_mae: 551.5818\n",
            "Epoch 49/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 10878.5186 - mae: 84.3259 - val_loss: 422593.3438 - val_mae: 642.7913\n",
            "Epoch 50/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 10677.1426 - mae: 82.1026 - val_loss: 953468.0625 - val_mae: 971.6741\n",
            "[953468.3125, 971.674072265625]\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer1 (Conv1D)              (None, 46, 100)           25100     \n",
            "_________________________________________________________________\n",
            "layer2 (BatchNormalization)  (None, 46, 100)           400       \n",
            "_________________________________________________________________\n",
            "layer3 (MaxPooling1D)        (None, 9, 100)            0         \n",
            "_________________________________________________________________\n",
            "layer4 (Conv1D)              (None, 5, 75)             37575     \n",
            "_________________________________________________________________\n",
            "layer5 (BatchNormalization)  (None, 5, 75)             300       \n",
            "_________________________________________________________________\n",
            "layer6 (Dropout)             (None, 5, 75)             0         \n",
            "_________________________________________________________________\n",
            "layer7 (MaxPooling1D)        (None, 1, 75)             0         \n",
            "_________________________________________________________________\n",
            "layer8 (Flatten)             (None, 75)                0         \n",
            "_________________________________________________________________\n",
            "layer9 (Dense)               (None, 1)                 76        \n",
            "=================================================================\n",
            "Total params: 63,451\n",
            "Trainable params: 63,101\n",
            "Non-trainable params: 350\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 2719170.5000 - mae: 1644.7158 - val_loss: 2229974.5000 - val_mae: 1490.0515\n",
            "Epoch 2/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 1402406.6250 - mae: 1169.1165 - val_loss: 1537268.6250 - val_mae: 1230.9464\n",
            "Epoch 3/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 383695.0625 - mae: 587.4661 - val_loss: 562160.3750 - val_mae: 716.7920\n",
            "Epoch 4/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 55876.8086 - mae: 196.5972 - val_loss: 313230.8125 - val_mae: 510.2538\n",
            "Epoch 5/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 22992.0195 - mae: 122.0071 - val_loss: 345649.5938 - val_mae: 543.9008\n",
            "Epoch 6/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 21600.1074 - mae: 117.4765 - val_loss: 464340.0000 - val_mae: 644.5975\n",
            "Epoch 7/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 23311.9316 - mae: 121.1313 - val_loss: 839601.8750 - val_mae: 890.3676\n",
            "Epoch 8/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 20590.2754 - mae: 113.6888 - val_loss: 408103.5000 - val_mae: 602.9240\n",
            "Epoch 9/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 24292.9785 - mae: 124.1740 - val_loss: 337687.7500 - val_mae: 540.9406\n",
            "Epoch 10/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 20481.5195 - mae: 114.7591 - val_loss: 327007.0312 - val_mae: 531.4718\n",
            "Epoch 11/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 22857.3672 - mae: 118.3869 - val_loss: 431263.2500 - val_mae: 623.2532\n",
            "Epoch 12/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 22121.5371 - mae: 119.4538 - val_loss: 456081.0938 - val_mae: 643.6440\n",
            "Epoch 13/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 19873.5430 - mae: 112.3686 - val_loss: 586361.8750 - val_mae: 740.5419\n",
            "Epoch 14/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 19429.7520 - mae: 111.8620 - val_loss: 314398.2188 - val_mae: 525.4869\n",
            "Epoch 15/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 20495.8691 - mae: 114.2395 - val_loss: 214069.1406 - val_mae: 420.2405\n",
            "Epoch 16/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 20223.9082 - mae: 112.6846 - val_loss: 159349.3281 - val_mae: 356.6546\n",
            "Epoch 17/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 19643.0977 - mae: 110.5891 - val_loss: 273691.2812 - val_mae: 490.1357\n",
            "Epoch 18/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 19150.8789 - mae: 112.6028 - val_loss: 120542.8281 - val_mae: 305.5224\n",
            "Epoch 19/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 17797.1055 - mae: 106.2774 - val_loss: 130743.7344 - val_mae: 320.9653\n",
            "Epoch 20/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 17709.3555 - mae: 106.3019 - val_loss: 58706.4062 - val_mae: 198.5921\n",
            "Epoch 21/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 17635.4609 - mae: 106.9288 - val_loss: 910538.0625 - val_mae: 937.7136\n",
            "Epoch 22/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 19735.1289 - mae: 113.1168 - val_loss: 513844.5938 - val_mae: 696.4380\n",
            "Epoch 23/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 17377.9062 - mae: 105.1390 - val_loss: 1906468.7500 - val_mae: 1370.8597\n",
            "Epoch 24/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 16936.5820 - mae: 102.7976 - val_loss: 29387.4336 - val_mae: 138.5427\n",
            "Epoch 25/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 18519.7480 - mae: 108.5713 - val_loss: 3987222.7500 - val_mae: 1991.2484\n",
            "Epoch 26/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 17962.7812 - mae: 107.3082 - val_loss: 1056651.0000 - val_mae: 1014.3760\n",
            "Epoch 27/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 16234.8926 - mae: 101.2502 - val_loss: 70593.8906 - val_mae: 222.8682\n",
            "Epoch 28/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 16444.7227 - mae: 103.3855 - val_loss: 1195140.7500 - val_mae: 1081.0392\n",
            "Epoch 29/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 16123.1611 - mae: 101.4910 - val_loss: 86214.4609 - val_mae: 256.6731\n",
            "Epoch 30/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 14501.1514 - mae: 97.0236 - val_loss: 507337.4688 - val_mae: 695.5603\n",
            "Epoch 31/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 14685.9336 - mae: 96.7891 - val_loss: 139019.6094 - val_mae: 339.4095\n",
            "Epoch 32/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 14559.9736 - mae: 95.9382 - val_loss: 531077.2500 - val_mae: 712.9169\n",
            "Epoch 33/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 14067.3535 - mae: 95.1119 - val_loss: 276675.3750 - val_mae: 505.5642\n",
            "Epoch 34/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 13315.7715 - mae: 91.2954 - val_loss: 141546.8438 - val_mae: 347.4863\n",
            "Epoch 35/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 13887.6562 - mae: 93.8483 - val_loss: 367548.1875 - val_mae: 589.5729\n",
            "Epoch 36/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 13852.1143 - mae: 93.9566 - val_loss: 2712871.5000 - val_mae: 1641.8599\n",
            "Epoch 37/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 13025.0732 - mae: 89.4614 - val_loss: 4149311.0000 - val_mae: 2033.2738\n",
            "Epoch 38/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 13290.3525 - mae: 91.3849 - val_loss: 2751581.2500 - val_mae: 1654.3147\n",
            "Epoch 39/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 12054.8945 - mae: 86.9169 - val_loss: 1355020.2500 - val_mae: 1158.0258\n",
            "Epoch 40/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 11454.0469 - mae: 85.4998 - val_loss: 65580.5312 - val_mae: 230.4850\n",
            "Epoch 41/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 12033.9648 - mae: 86.5186 - val_loss: 98412.9922 - val_mae: 291.9719\n",
            "Epoch 42/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 11982.2939 - mae: 88.1029 - val_loss: 95207.4688 - val_mae: 287.4801\n",
            "Epoch 43/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 11601.0840 - mae: 86.7495 - val_loss: 15609.9092 - val_mae: 104.1001\n",
            "Epoch 44/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 10135.4268 - mae: 79.6035 - val_loss: 18658.4219 - val_mae: 107.7649\n",
            "Epoch 45/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 10526.0186 - mae: 82.1499 - val_loss: 1524229.2500 - val_mae: 1229.5515\n",
            "Epoch 46/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 10093.7344 - mae: 80.0769 - val_loss: 46176.2539 - val_mae: 187.7478\n",
            "Epoch 47/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 9871.3271 - mae: 80.5919 - val_loss: 11592.8662 - val_mae: 88.2942\n",
            "Epoch 48/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 9241.1816 - mae: 76.3804 - val_loss: 1203917.2500 - val_mae: 1092.3406\n",
            "Epoch 49/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 10074.8799 - mae: 79.2594 - val_loss: 242362.9375 - val_mae: 481.2985\n",
            "Epoch 50/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 9523.6865 - mae: 76.9086 - val_loss: 30163.2070 - val_mae: 148.2706\n",
            "[30163.19140625, 148.2705078125]\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer1 (Conv1D)              (None, 46, 100)           25100     \n",
            "_________________________________________________________________\n",
            "layer2 (BatchNormalization)  (None, 46, 100)           400       \n",
            "_________________________________________________________________\n",
            "layer3 (MaxPooling1D)        (None, 9, 100)            0         \n",
            "_________________________________________________________________\n",
            "layer4 (Conv1D)              (None, 5, 75)             37575     \n",
            "_________________________________________________________________\n",
            "layer5 (BatchNormalization)  (None, 5, 75)             300       \n",
            "_________________________________________________________________\n",
            "layer6 (Dropout)             (None, 5, 75)             0         \n",
            "_________________________________________________________________\n",
            "layer7 (MaxPooling1D)        (None, 1, 75)             0         \n",
            "_________________________________________________________________\n",
            "layer8 (Flatten)             (None, 75)                0         \n",
            "_________________________________________________________________\n",
            "layer9 (Dense)               (None, 1)                 76        \n",
            "=================================================================\n",
            "Total params: 63,451\n",
            "Trainable params: 63,101\n",
            "Non-trainable params: 350\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 2791049.7500 - mae: 1667.3488 - val_loss: 2450478.2500 - val_mae: 1562.6017\n",
            "Epoch 2/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 1684095.2500 - mae: 1287.1884 - val_loss: 1838288.6250 - val_mae: 1350.0874\n",
            "Epoch 3/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 659356.8750 - mae: 789.5768 - val_loss: 1046611.8125 - val_mae: 1005.4689\n",
            "Epoch 4/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 154419.8906 - mae: 352.2682 - val_loss: 580653.6250 - val_mae: 729.3996\n",
            "Epoch 5/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 38296.2109 - mae: 155.2354 - val_loss: 415078.6250 - val_mae: 606.6218\n",
            "Epoch 6/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 27650.4922 - mae: 130.9621 - val_loss: 466107.3750 - val_mae: 648.7024\n",
            "Epoch 7/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 27455.0410 - mae: 133.7068 - val_loss: 499007.0625 - val_mae: 676.3911\n",
            "Epoch 8/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 27412.6191 - mae: 131.3310 - val_loss: 525569.3125 - val_mae: 697.3663\n",
            "Epoch 9/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 26886.1797 - mae: 130.0605 - val_loss: 443461.8125 - val_mae: 635.1487\n",
            "Epoch 10/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 27878.8340 - mae: 133.2584 - val_loss: 514015.3750 - val_mae: 687.9846\n",
            "Epoch 11/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 24285.0781 - mae: 123.9857 - val_loss: 589508.8750 - val_mae: 742.7422\n",
            "Epoch 12/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 24593.6816 - mae: 125.2501 - val_loss: 373624.3125 - val_mae: 579.1436\n",
            "Epoch 13/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 25856.0703 - mae: 128.2111 - val_loss: 518965.4688 - val_mae: 693.7186\n",
            "Epoch 14/50\n",
            "857/857 [==============================] - 4s 4ms/step - loss: 23674.1543 - mae: 123.3321 - val_loss: 116180.7109 - val_mae: 297.0191\n",
            "Epoch 15/50\n",
            "857/857 [==============================] - 4s 5ms/step - loss: 24243.6445 - mae: 124.6020 - val_loss: 308027.3750 - val_mae: 523.6404\n",
            "Epoch 16/50\n",
            "857/857 [==============================] - 4s 4ms/step - loss: 22747.4023 - mae: 120.5423 - val_loss: 231339.7969 - val_mae: 445.8594\n",
            "Epoch 17/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 22915.4629 - mae: 120.6258 - val_loss: 30569.1855 - val_mae: 135.8950\n",
            "Epoch 18/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 22300.1172 - mae: 120.6285 - val_loss: 1135551.6250 - val_mae: 1051.3379\n",
            "Epoch 19/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 21527.1465 - mae: 117.7466 - val_loss: 128628.1250 - val_mae: 320.3512\n",
            "Epoch 20/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 22058.0195 - mae: 118.0853 - val_loss: 40947.7734 - val_mae: 163.7219\n",
            "Epoch 21/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 23215.3555 - mae: 122.4983 - val_loss: 826122.4375 - val_mae: 894.3669\n",
            "Epoch 22/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 23869.2949 - mae: 122.8961 - val_loss: 620884.5625 - val_mae: 772.2077\n",
            "Epoch 23/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 21024.3965 - mae: 115.0245 - val_loss: 1152403.7500 - val_mae: 1063.0409\n",
            "Epoch 24/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 21285.9023 - mae: 116.3480 - val_loss: 384014.2812 - val_mae: 600.3613\n",
            "Epoch 25/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 20666.6797 - mae: 114.8356 - val_loss: 272955.7500 - val_mae: 500.6802\n",
            "Epoch 26/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 21582.9473 - mae: 116.9744 - val_loss: 87988.3828 - val_mae: 264.6750\n",
            "Epoch 27/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 20974.5020 - mae: 115.7764 - val_loss: 646379.2500 - val_mae: 790.3571\n",
            "Epoch 28/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 19747.6914 - mae: 111.4331 - val_loss: 290003.7188 - val_mae: 520.3711\n",
            "Epoch 29/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 17967.4043 - mae: 106.0042 - val_loss: 293218.3125 - val_mae: 522.7695\n",
            "Epoch 30/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 20991.6152 - mae: 116.1656 - val_loss: 134068.5312 - val_mae: 339.7414\n",
            "Epoch 31/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 17432.2676 - mae: 106.4485 - val_loss: 387952.6562 - val_mae: 607.8766\n",
            "Epoch 32/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 18925.0234 - mae: 109.3379 - val_loss: 358361.4375 - val_mae: 582.6486\n",
            "Epoch 33/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 17705.4609 - mae: 105.5519 - val_loss: 20264.4219 - val_mae: 113.6789\n",
            "Epoch 34/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 18319.5293 - mae: 107.1145 - val_loss: 428477.6875 - val_mae: 640.5380\n",
            "Epoch 35/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 16084.4092 - mae: 100.3890 - val_loss: 138408.5312 - val_mae: 347.6171\n",
            "Epoch 36/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 16613.2090 - mae: 100.3285 - val_loss: 899612.0000 - val_mae: 939.9678\n",
            "Epoch 37/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 15993.7109 - mae: 101.0651 - val_loss: 115692.8594 - val_mae: 317.6536\n",
            "Epoch 38/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 15543.3584 - mae: 98.2832 - val_loss: 996294.6875 - val_mae: 990.5638\n",
            "Epoch 39/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 14375.2197 - mae: 96.9357 - val_loss: 508261.7188 - val_mae: 703.5338\n",
            "Epoch 40/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 14710.7432 - mae: 97.5158 - val_loss: 890145.0625 - val_mae: 936.6053\n",
            "Epoch 41/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 13702.1367 - mae: 92.8838 - val_loss: 240799.7812 - val_mae: 477.8603\n",
            "Epoch 42/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 14716.5586 - mae: 97.4877 - val_loss: 56638.5547 - val_mae: 212.0598\n",
            "Epoch 43/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 12686.8604 - mae: 89.2905 - val_loss: 439412.1250 - val_mae: 653.7267\n",
            "Epoch 44/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 12784.6484 - mae: 91.4688 - val_loss: 263025.9375 - val_mae: 501.0430\n",
            "Epoch 45/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 12807.4668 - mae: 90.0910 - val_loss: 360972.5938 - val_mae: 590.9320\n",
            "Epoch 46/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 13336.7920 - mae: 92.1819 - val_loss: 810261.8750 - val_mae: 893.8903\n",
            "Epoch 47/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 10619.7109 - mae: 80.8364 - val_loss: 224640.7344 - val_mae: 462.0072\n",
            "Epoch 48/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 12847.2402 - mae: 90.3873 - val_loss: 533779.4375 - val_mae: 723.0643\n",
            "Epoch 49/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 11376.5645 - mae: 84.8564 - val_loss: 721180.6875 - val_mae: 842.9454\n",
            "Epoch 50/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 10897.9150 - mae: 82.6046 - val_loss: 321993.7812 - val_mae: 558.1836\n",
            "[321993.875, 558.18359375]\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer1 (Conv1D)              (None, 46, 100)           25100     \n",
            "_________________________________________________________________\n",
            "layer2 (BatchNormalization)  (None, 46, 100)           400       \n",
            "_________________________________________________________________\n",
            "layer3 (MaxPooling1D)        (None, 9, 100)            0         \n",
            "_________________________________________________________________\n",
            "layer4 (Conv1D)              (None, 5, 75)             37575     \n",
            "_________________________________________________________________\n",
            "layer5 (BatchNormalization)  (None, 5, 75)             300       \n",
            "_________________________________________________________________\n",
            "layer6 (Dropout)             (None, 5, 75)             0         \n",
            "_________________________________________________________________\n",
            "layer7 (MaxPooling1D)        (None, 1, 75)             0         \n",
            "_________________________________________________________________\n",
            "layer8 (Flatten)             (None, 75)                0         \n",
            "_________________________________________________________________\n",
            "layer9 (Dense)               (None, 1)                 76        \n",
            "=================================================================\n",
            "Total params: 63,451\n",
            "Trainable params: 63,101\n",
            "Non-trainable params: 350\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 2790601.7500 - mae: 1667.1854 - val_loss: 2321356.0000 - val_mae: 1521.3392\n",
            "Epoch 2/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 1712543.8750 - mae: 1298.9943 - val_loss: 1605540.8750 - val_mae: 1263.4485\n",
            "Epoch 3/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 672688.0625 - mae: 798.4279 - val_loss: 1114614.5000 - val_mae: 1048.5852\n",
            "Epoch 4/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 165966.9688 - mae: 367.9146 - val_loss: 581919.7500 - val_mae: 749.2068\n",
            "Epoch 5/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 37178.3086 - mae: 154.7254 - val_loss: 457057.1250 - val_mae: 659.3091\n",
            "Epoch 6/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 26641.0977 - mae: 130.1125 - val_loss: 428107.9688 - val_mae: 637.9060\n",
            "Epoch 7/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 27556.8633 - mae: 133.8378 - val_loss: 359057.0938 - val_mae: 581.9153\n",
            "Epoch 8/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 25920.1094 - mae: 129.2135 - val_loss: 643254.9375 - val_mae: 791.7517\n",
            "Epoch 9/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 25968.8145 - mae: 129.9976 - val_loss: 395173.1875 - val_mae: 614.9299\n",
            "Epoch 10/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 25120.3066 - mae: 126.5333 - val_loss: 354608.9688 - val_mae: 581.2718\n",
            "Epoch 11/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 24840.4316 - mae: 126.4787 - val_loss: 409458.2812 - val_mae: 626.8923\n",
            "Epoch 12/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 24720.1875 - mae: 123.7209 - val_loss: 168374.6875 - val_mae: 390.1103\n",
            "Epoch 13/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 25094.3262 - mae: 125.7999 - val_loss: 876243.2500 - val_mae: 927.3345\n",
            "Epoch 14/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 21765.0996 - mae: 118.3877 - val_loss: 254935.7500 - val_mae: 488.9099\n",
            "Epoch 15/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 21650.6816 - mae: 117.4787 - val_loss: 326744.8750 - val_mae: 558.5625\n",
            "Epoch 16/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 23888.4512 - mae: 124.1378 - val_loss: 860770.9375 - val_mae: 919.3436\n",
            "Epoch 17/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 22560.5039 - mae: 120.9050 - val_loss: 463806.2188 - val_mae: 669.9199\n",
            "Epoch 18/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 21868.2109 - mae: 117.4412 - val_loss: 5649218.0000 - val_mae: 2374.7664\n",
            "Epoch 19/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 21951.1777 - mae: 119.3620 - val_loss: 249374.8281 - val_mae: 480.7476\n",
            "Epoch 20/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 19470.5273 - mae: 111.4650 - val_loss: 1702190.2500 - val_mae: 1299.2927\n",
            "Epoch 21/50\n",
            "857/857 [==============================] - 3s 3ms/step - loss: 19934.4805 - mae: 113.4075 - val_loss: 1651340.5000 - val_mae: 1280.1832\n",
            "Epoch 22/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 19946.1113 - mae: 113.0128 - val_loss: 19672.9766 - val_mae: 109.6499\n",
            "Epoch 23/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 18921.6738 - mae: 108.5166 - val_loss: 789494.3750 - val_mae: 880.8591\n",
            "Epoch 24/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 18351.3926 - mae: 109.2308 - val_loss: 47153.6055 - val_mae: 190.1643\n",
            "Epoch 25/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 17599.4922 - mae: 104.5249 - val_loss: 173161.2031 - val_mae: 398.8583\n",
            "Epoch 26/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 17962.9727 - mae: 107.5228 - val_loss: 827827.4375 - val_mae: 902.5507\n",
            "Epoch 27/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 18402.5176 - mae: 107.7147 - val_loss: 901643.1250 - val_mae: 942.7956\n",
            "Epoch 28/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 16277.4199 - mae: 103.0458 - val_loss: 1800778.8750 - val_mae: 1337.8250\n",
            "Epoch 29/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 16884.1094 - mae: 104.5015 - val_loss: 1546102.6250 - val_mae: 1238.7787\n",
            "Epoch 30/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 17498.1641 - mae: 105.9315 - val_loss: 1183797.6250 - val_mae: 1082.7585\n",
            "Epoch 31/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 15432.6533 - mae: 98.6348 - val_loss: 288692.2188 - val_mae: 526.5349\n",
            "Epoch 32/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 15013.2803 - mae: 97.6934 - val_loss: 32411.9121 - val_mae: 150.5551\n",
            "Epoch 33/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 15822.6748 - mae: 100.6229 - val_loss: 21020.6660 - val_mae: 117.8214\n",
            "Epoch 34/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 15170.2148 - mae: 98.8640 - val_loss: 1523882.7500 - val_mae: 1230.0751\n",
            "Epoch 35/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 14637.3789 - mae: 96.5230 - val_loss: 36005.9727 - val_mae: 166.1463\n",
            "Epoch 36/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 13297.6182 - mae: 92.4731 - val_loss: 482729.6250 - val_mae: 687.6930\n",
            "Epoch 37/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 14027.4678 - mae: 93.7710 - val_loss: 3230442.7500 - val_mae: 1794.7992\n",
            "Epoch 38/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 13001.8145 - mae: 90.4279 - val_loss: 1387505.1250 - val_mae: 1174.0713\n",
            "Epoch 39/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 12057.6406 - mae: 87.2728 - val_loss: 574378.0000 - val_mae: 751.9479\n",
            "Epoch 40/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 12984.8809 - mae: 90.3984 - val_loss: 473357.9375 - val_mae: 681.7555\n",
            "Epoch 41/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 11905.2812 - mae: 86.5530 - val_loss: 219367.2812 - val_mae: 458.8225\n",
            "Epoch 42/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 12330.5801 - mae: 88.1996 - val_loss: 1127494.1250 - val_mae: 1057.8645\n",
            "Epoch 43/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 11316.1504 - mae: 84.8784 - val_loss: 503661.7500 - val_mae: 703.5312\n",
            "Epoch 44/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 10945.4121 - mae: 83.6005 - val_loss: 1034214.2500 - val_mae: 1013.2468\n",
            "Epoch 45/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 11482.7959 - mae: 85.4694 - val_loss: 1311696.5000 - val_mae: 1141.9568\n",
            "Epoch 46/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 10639.1973 - mae: 82.3259 - val_loss: 896778.6875 - val_mae: 942.9772\n",
            "Epoch 47/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 9546.0352 - mae: 77.8429 - val_loss: 908802.0625 - val_mae: 949.3043\n",
            "Epoch 48/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 9355.7314 - mae: 75.9180 - val_loss: 497923.8750 - val_mae: 700.3002\n",
            "Epoch 49/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 9828.6084 - mae: 80.5252 - val_loss: 10452.2656 - val_mae: 83.1732\n",
            "Epoch 50/50\n",
            "857/857 [==============================] - 2s 3ms/step - loss: 9791.0518 - mae: 77.9761 - val_loss: 57191.5195 - val_mae: 223.5184\n",
            "[57191.4765625, 223.51834106445312]\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer1 (Conv1D)              (None, 46, 100)           25100     \n",
            "_________________________________________________________________\n",
            "layer2 (BatchNormalization)  (None, 46, 100)           400       \n",
            "_________________________________________________________________\n",
            "layer3 (MaxPooling1D)        (None, 9, 100)            0         \n",
            "_________________________________________________________________\n",
            "layer4 (Conv1D)              (None, 5, 75)             37575     \n",
            "_________________________________________________________________\n",
            "layer5 (BatchNormalization)  (None, 5, 75)             300       \n",
            "_________________________________________________________________\n",
            "layer6 (Dropout)             (None, 5, 75)             0         \n",
            "_________________________________________________________________\n",
            "layer7 (MaxPooling1D)        (None, 1, 75)             0         \n",
            "_________________________________________________________________\n",
            "layer8 (Flatten)             (None, 75)                0         \n",
            "_________________________________________________________________\n",
            "layer9 (Dense)               (None, 1)                 76        \n",
            "=================================================================\n",
            "Total params: 63,451\n",
            "Trainable params: 63,101\n",
            "Non-trainable params: 350\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "858/858 [==============================] - 3s 3ms/step - loss: 2735036.2500 - mae: 1649.7194 - val_loss: 2202478.5000 - val_mae: 1481.4186\n",
            "Epoch 2/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 1457458.6250 - mae: 1192.4089 - val_loss: 1698146.3750 - val_mae: 1296.8174\n",
            "Epoch 3/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 428800.7812 - mae: 624.2218 - val_loss: 699528.3750 - val_mae: 813.6310\n",
            "Epoch 4/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 74307.3828 - mae: 230.5080 - val_loss: 417812.0625 - val_mae: 607.0018\n",
            "Epoch 5/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 28341.3887 - mae: 134.8914 - val_loss: 317091.0938 - val_mae: 527.3976\n",
            "Epoch 6/50\n",
            "858/858 [==============================] - 3s 3ms/step - loss: 24473.6348 - mae: 124.2965 - val_loss: 397708.5625 - val_mae: 596.4800\n",
            "Epoch 7/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 22556.8047 - mae: 121.0261 - val_loss: 265202.3750 - val_mae: 475.4631\n",
            "Epoch 8/50\n",
            "858/858 [==============================] - 3s 3ms/step - loss: 24772.8672 - mae: 125.7982 - val_loss: 403857.0312 - val_mae: 604.8085\n",
            "Epoch 9/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 24497.5566 - mae: 125.7907 - val_loss: 356144.6250 - val_mae: 566.0092\n",
            "Epoch 10/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 24599.6133 - mae: 126.0503 - val_loss: 324657.5312 - val_mae: 536.5990\n",
            "Epoch 11/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 24028.8027 - mae: 123.6992 - val_loss: 171341.5625 - val_mae: 373.8507\n",
            "Epoch 12/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 22789.7109 - mae: 121.8165 - val_loss: 543550.9375 - val_mae: 713.4423\n",
            "Epoch 13/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 21650.2305 - mae: 119.5563 - val_loss: 263608.8750 - val_mae: 479.4738\n",
            "Epoch 14/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 22074.1074 - mae: 116.7712 - val_loss: 1388980.1250 - val_mae: 1164.9216\n",
            "Epoch 15/50\n",
            "858/858 [==============================] - 3s 3ms/step - loss: 22989.8477 - mae: 121.8286 - val_loss: 202472.8750 - val_mae: 415.1646\n",
            "Epoch 16/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 21437.9355 - mae: 117.3094 - val_loss: 604787.1250 - val_mae: 758.0031\n",
            "Epoch 17/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 21311.7480 - mae: 117.2624 - val_loss: 1559040.6250 - val_mae: 1236.6613\n",
            "Epoch 18/50\n",
            "858/858 [==============================] - 3s 3ms/step - loss: 18737.8320 - mae: 109.2556 - val_loss: 110422.2031 - val_mae: 290.4928\n",
            "Epoch 19/50\n",
            "858/858 [==============================] - 3s 3ms/step - loss: 20138.7441 - mae: 113.6678 - val_loss: 1053807.2500 - val_mae: 1013.9242\n",
            "Epoch 20/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 20753.8711 - mae: 114.2473 - val_loss: 137784.7344 - val_mae: 340.8421\n",
            "Epoch 21/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 20169.9590 - mae: 113.8126 - val_loss: 26698.2109 - val_mae: 128.0504\n",
            "Epoch 22/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 21379.2969 - mae: 115.9704 - val_loss: 48889.7031 - val_mae: 188.2618\n",
            "Epoch 23/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 21209.8711 - mae: 116.8495 - val_loss: 2209886.7500 - val_mae: 1478.4365\n",
            "Epoch 24/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 18496.4082 - mae: 109.1987 - val_loss: 132787.8281 - val_mae: 328.7703\n",
            "Epoch 25/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 18985.9824 - mae: 112.2121 - val_loss: 697529.1250 - val_mae: 822.0067\n",
            "Epoch 26/50\n",
            "858/858 [==============================] - 3s 3ms/step - loss: 19986.7305 - mae: 112.5096 - val_loss: 417320.3750 - val_mae: 627.8613\n",
            "Epoch 27/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 19457.3398 - mae: 111.1410 - val_loss: 3472020.7500 - val_mae: 1857.7629\n",
            "Epoch 28/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 16995.5156 - mae: 104.2397 - val_loss: 2027759.1250 - val_mae: 1417.1062\n",
            "Epoch 29/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 18950.5371 - mae: 110.0355 - val_loss: 1235066.3750 - val_mae: 1102.3530\n",
            "Epoch 30/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 17341.2148 - mae: 104.0308 - val_loss: 132222.0000 - val_mae: 337.5536\n",
            "Epoch 31/50\n",
            "858/858 [==============================] - 3s 3ms/step - loss: 16812.6680 - mae: 102.3172 - val_loss: 1096496.1250 - val_mae: 1038.5475\n",
            "Epoch 32/50\n",
            "858/858 [==============================] - 3s 3ms/step - loss: 17028.4492 - mae: 102.9961 - val_loss: 2581965.7500 - val_mae: 1601.5570\n",
            "Epoch 33/50\n",
            "858/858 [==============================] - 3s 3ms/step - loss: 15970.1094 - mae: 101.3860 - val_loss: 304074.3125 - val_mae: 534.0063\n",
            "Epoch 34/50\n",
            "858/858 [==============================] - 3s 3ms/step - loss: 14794.0479 - mae: 98.0891 - val_loss: 1723802.5000 - val_mae: 1306.0341\n",
            "Epoch 35/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 15364.9404 - mae: 97.7254 - val_loss: 395253.1875 - val_mae: 614.8656\n",
            "Epoch 36/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 16431.1641 - mae: 99.8694 - val_loss: 1515593.1250 - val_mae: 1224.9401\n",
            "Epoch 37/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 14640.2646 - mae: 98.2222 - val_loss: 785826.3750 - val_mae: 878.1597\n",
            "Epoch 38/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 15106.0254 - mae: 95.7337 - val_loss: 443636.6250 - val_mae: 655.4495\n",
            "Epoch 39/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 15148.1465 - mae: 100.0900 - val_loss: 46704.7227 - val_mae: 191.9984\n",
            "Epoch 40/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 14269.1689 - mae: 93.5524 - val_loss: 366147.8438 - val_mae: 593.7598\n",
            "Epoch 41/50\n",
            "858/858 [==============================] - 3s 3ms/step - loss: 15301.9316 - mae: 98.3992 - val_loss: 1737394.3750 - val_mae: 1312.7322\n",
            "Epoch 42/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 13926.6240 - mae: 95.3899 - val_loss: 451558.2500 - val_mae: 661.4223\n",
            "Epoch 43/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 12649.6768 - mae: 90.6794 - val_loss: 892269.8125 - val_mae: 937.4941\n",
            "Epoch 44/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 13446.3027 - mae: 90.8255 - val_loss: 165489.5312 - val_mae: 389.6157\n",
            "Epoch 45/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 12847.3389 - mae: 91.2757 - val_loss: 116567.9609 - val_mae: 323.1598\n",
            "Epoch 46/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 12620.7002 - mae: 91.6024 - val_loss: 891219.0625 - val_mae: 937.6833\n",
            "Epoch 47/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 12759.4170 - mae: 90.5348 - val_loss: 1922800.5000 - val_mae: 1382.3087\n",
            "Epoch 48/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 12760.6641 - mae: 88.8916 - val_loss: 87402.1094 - val_mae: 276.3304\n",
            "Epoch 49/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 12034.9775 - mae: 85.9901 - val_loss: 129727.0547 - val_mae: 343.3996\n",
            "Epoch 50/50\n",
            "858/858 [==============================] - 2s 3ms/step - loss: 11807.2510 - mae: 86.1708 - val_loss: 739851.8750 - val_mae: 853.1119\n",
            "[739852.0, 853.1119995117188]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VBBWdRDhA73",
        "colab_type": "text"
      },
      "source": [
        "Printing the Loss Score "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N5s1lAgnh_e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "deeb87da-70c6-4150-b014-2db92be61596"
      },
      "source": [
        "print('-----------------------------CNN------------------------------------------')\n",
        "print('Score per fold for cnn')\n",
        "for i in range(0, len(k_fold_acc_cnn)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {k_fold_loss_cnn[i]} - Accuracy: {k_fold_acc_cnn[i]}')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds for cnn:')\n",
        "print(f'> Accuracy: {np.mean(k_fold_acc_cnn)} (+- {np.std(k_fold_acc_cnn)})')\n",
        "print(f'> Loss: {np.mean(k_fold_loss_cnn)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------CNN------------------------------------------\n",
            "Score per fold for cnn\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 88762.390625 - Accuracy: 28421.267700195312\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 30553.345703125 - Accuracy: 14929.47998046875\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 953468.3125 - Accuracy: 97167.4072265625\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 30163.19140625 - Accuracy: 14827.05078125\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 321993.875 - Accuracy: 55818.359375\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 57191.4765625 - Accuracy: 22351.834106445312\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 739852.0 - Accuracy: 85311.19995117188\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds for cnn:\n",
            "> Accuracy: 45546.657017299105 (+- 31766.588697567437)\n",
            "> Loss: 317426.3702566964\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSe7GBYFS-9K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "f381d4f0-a57c-4c0f-dc66-bdb9676b6caa"
      },
      "source": [
        "#Graph for CNN model's loss (Validation loss and training loss)\n",
        "plt.plot(history_cnn.history['loss'])\n",
        "plt.plot(history_cnn.history['val_loss'])\n",
        "plt.title('model loss cnn')\n",
        "plt.ylabel('loss_cnn')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "plt.savefig('loss_cnn.png')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-RwpBFChRki",
        "colab_type": "text"
      },
      "source": [
        "Loading the CNN model and evaluating the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGJBF8fuaqfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load model\n",
        "\n",
        "modelnew_cnn = tf.keras.models.load_model('1106937_CNN.h5')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRwASaKtaytV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "ba0425e3-ed69-40ec-94ae-e1ed617774c3"
      },
      "source": [
        "modelnew_cnn.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer1 (Conv1D)              (None, 46, 100)           25100     \n",
            "_________________________________________________________________\n",
            "layer2 (BatchNormalization)  (None, 46, 100)           400       \n",
            "_________________________________________________________________\n",
            "layer3 (MaxPooling1D)        (None, 9, 100)            0         \n",
            "_________________________________________________________________\n",
            "layer4 (Conv1D)              (None, 5, 75)             37575     \n",
            "_________________________________________________________________\n",
            "layer5 (BatchNormalization)  (None, 5, 75)             300       \n",
            "_________________________________________________________________\n",
            "layer6 (Dropout)             (None, 5, 75)             0         \n",
            "_________________________________________________________________\n",
            "layer7 (MaxPooling1D)        (None, 1, 75)             0         \n",
            "_________________________________________________________________\n",
            "layer8 (Flatten)             (None, 75)                0         \n",
            "_________________________________________________________________\n",
            "layer9 (Dense)               (None, 1)                 76        \n",
            "=================================================================\n",
            "Total params: 63,451\n",
            "Trainable params: 63,101\n",
            "Non-trainable params: 350\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdL7iwtBa4I1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scoresnew_cnn = modelnew_cnn.evaluate(scale_input_cnn[test], data_target_cnn[test], verbose=0)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUWeFKpoa990",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0bb12448-b72b-4d6a-ea0f-10307840302e"
      },
      "source": [
        "# Printing the values\n",
        "print(scoresnew_cnn)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[739852.0, 853.1119995117188]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWrKODp_hj20",
        "colab_type": "text"
      },
      "source": [
        "***Quetion 5***\n",
        "Creating the graph for the performance difference between Loss for ANN and CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOAsopxkn9iX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "7df8d83b-1e24-4417-bb17-6545fab76cc4"
      },
      "source": [
        "plt.plot(history_ann.history['loss'])\n",
        "plt.plot(history_ann.history['val_loss'])\n",
        "plt.plot(history_cnn.history['loss'])\n",
        "plt.plot(history_cnn.history['val_loss'])\n",
        "plt.title('model loss ann')\n",
        "plt.ylabel('loss_ann_cnn')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train_loss_ann', 'val_loss_ann','train_loss_cnn', 'val_loss_cnn'], loc='upper left')\n",
        "plt.show()\n",
        "plt.savefig('loss_comparison.png')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1KGGSKjYop4",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "From the above graph we can see that CNN loss is varying when compared to ANN. The below comparison is with all the values calculated by its mean for both ANN and CNN. For all values, CNN shows a higher loss value. Comparing the results of ANN and CNN, we can conclude that CNN is better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbc1MVoPTKhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mae_ann = np.mean(history_ann.history['mae'])\n",
        "val_mae_ann = np.mean(history_ann.history['val_mae'])\n",
        "mae_cnn = np.mean(history_cnn.history['mae'])\n",
        "val_mae_cnn = np.mean(history_cnn.history['val_mae'])\n",
        "loss_ann = np.mean(history_ann.history['loss'])\n",
        "val_loss_ann = np.mean(history_ann.history['val_loss'])\n",
        "loss_cnn = np.mean(history_cnn.history['mae'])\n",
        "val_loss_cnn = np.mean(history_cnn.history['val_loss'])"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwET7L7uU8kr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "4be41647-1c6f-4ab8-9854-b70aef80acad"
      },
      "source": [
        "print(f'> mae ANN: {mae_ann}')\n",
        "print(f'> Val mae ANN: {val_mae_ann}')\n",
        "print(f'> mae CNN: {mae_cnn}')\n",
        "print(f'> Val mae CNN: {val_mae_cnn}')\n",
        "print(f'> Loss ANN: {loss_ann}')\n",
        "print(f'> Val Loss ANN: {val_loss_ann}')\n",
        "print(f'> Loss CNN: {loss_cnn}')\n",
        "print(f'> Val Loss CNN: {val_loss_cnn}')\n",
        "print(f'> Loss ANN: {np.mean(k_fold_loss_ann)}')\n",
        "print(f'> Loss CNN: {np.mean(k_fold_loss_cnn)}')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> mae ANN: 33.86211605072022\n",
            "> Val mae ANN: 78.29010833740234\n",
            "> mae CNN: 172.79733032226562\n",
            "> Val mae CNN: 772.7428283691406\n",
            "> Loss ANN: 1993.9657318738043\n",
            "> Val Loss ANN: 9698.657346141581\n",
            "> Loss CNN: 172.79733032226562\n",
            "> Val Loss CNN: 769589.9119100765\n",
            "> Loss ANN: 11468.08189174107\n",
            "> Loss CNN: 317426.3702566964\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}